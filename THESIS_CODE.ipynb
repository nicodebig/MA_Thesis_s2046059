{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"HF_TOKEN\"] = \"HF_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation\n",
    "\n",
    "This section contains the code that was used to generate the RESPONSible Service dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements_3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client with API key, API version, and deployment endpoint.\n",
    "client = AzureOpenAI(\n",
    "            api_key=\"AZUREOPENAI_KEY\", # Your Azure OpenAI API key (keep this secret!)\n",
    "            api_version=\"2024-08-01-preview\", # API version to use\n",
    "            azure_endpoint=\"https://openai-ds-instance-sweden.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\"\n",
    "        )\n",
    "\n",
    "# Name of the deployed model\n",
    "deployment_name='gpt-4o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to collect generated dataset entries\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of each dataset entry using a Pydantic model for validation\n",
    "class DatasetGeneration(BaseModel):\n",
    "    request: str\n",
    "    response0: str\n",
    "    response1: str\n",
    "\n",
    "# Number of dataset examples to generate\n",
    "num_samples = 1000\n",
    "\n",
    "# Target communication style for response generation\n",
    "core_value = \"polite\"\n",
    "\n",
    "# Loop to generate synthetic data\n",
    "for i in range(num_samples):\n",
    "    # Predefined list of customer service topics\n",
    "    topics = [\"orders\", \"refunds\", \"billing\", \"subscriptions\", \"account\", \"shipping\",\n",
    "              \"feedbacks\", \"complaints\", \"product information\", \"technical assistance\", \"discounts\"]\n",
    "\n",
    "    # Randomly choose a topic for this example\n",
    "    random_topic = random.choice(topics)\n",
    "\n",
    "    # Prompt instructing the model to generate a dataset entry with 30-word fields\n",
    "    prompt = f\"\"\"\n",
    "    Create one dataset entry. The entry should include the following fields:\n",
    "    - request: Request for assistance from a customer to customer service about {random_topic}. Generate exactly 30 words.\n",
    "    - response0: Response from customer service to the customer's request. response0 must be phrased in a {core_value} way. Generate exactly 30 words.\n",
    "    - response1: Response from customer service to the customer's request. response1 must be phrased in a more {core_value} way than response0. Generate exactly 30 words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the request to the Azure OpenAI service and parse the structured response\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=1.0,               # High temperature for creative output\n",
    "        top_p=1.0,                     # Full probability mass\n",
    "        response_format=DatasetGeneration,  # Use the Pydantic schema for structured parsing\n",
    "    )\n",
    "\n",
    "    # Extract the parsed output from the model's response\n",
    "    generated_example = completion.choices[0].message.parsed\n",
    "\n",
    "    # Convert the structured output to a dictionary and add to dataset\n",
    "    generated_example_dict = generated_example.model_dump()\n",
    "    dataset.append(generated_example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to a DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Write DataFrame to a Parquet file\n",
    "df.to_parquet(f\"dataset_{core_value}.parquet\", index=False)\n",
    "\n",
    "print(\"Parquet dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Evaluation\n",
    "\n",
    "Ths section contains the script to process as .csv files the RESPONSible Service dataset that was just generated, so that it can be manually annotated and then re-uploaded to calculate human ground truth agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "def process_parquet_to_csv(parquet_file: str, output_folder: str = \"hum_eval_dataset\", sample_size: int = 1000):\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "\n",
    "    # Load the Parquet file\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    # Sample rows\n",
    "    df = df[[\"request\", \"response0\", \"response1\"]]\n",
    "    df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Swap responses randomly and assign labels\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        if random.random() < 0.5:\n",
    "            rows.append({\n",
    "                \"request\": row[\"request\"],\n",
    "                \"response0\": row[\"response0\"],\n",
    "                \"response1\": row[\"response1\"],\n",
    "                \"label\": 1\n",
    "            })\n",
    "        else:\n",
    "            rows.append({\n",
    "                \"request\": row[\"request\"],\n",
    "                \"response0\": row[\"response1\"],\n",
    "                \"response1\": row[\"response0\"],\n",
    "                \"label\": 0\n",
    "            })\n",
    "\n",
    "    df_final = pd.DataFrame(rows)\n",
    "\n",
    "    # Separate label column\n",
    "    labels = df_final[\"label\"]\n",
    "    df_final = df_final.drop(columns=[\"label\"])\n",
    "    df_final[\"human\"] = \"\"\n",
    "\n",
    "    # Prepare output filenames\n",
    "    base_name = os.path.splitext(os.path.basename(parquet_file))[0]\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    output_path = os.path.join(output_folder, f\"{base_name}.csv\")\n",
    "    label_path = os.path.join(output_folder, f\"{base_name}_labels.csv\")\n",
    "\n",
    "    # Save the CSVs\n",
    "    df_final.to_csv(output_path, index=False, encoding=\"utf-8\", sep=\";\")\n",
    "    pd.DataFrame({\"label\": labels}).to_csv(label_path, index=False, encoding=\"utf-8\", sep=\";\")\n",
    "\n",
    "    print(\"âœ… CSV export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_parquet_to_csv(\"RESPONSible Service/dataset_clear.parquet\")\n",
    "process_parquet_to_csv(\"RESPONSible Service/dataset_friendly.parquet\")\n",
    "process_parquet_to_csv(\"RESPONSible Service/dataset_empathetic.parquet\")\n",
    "process_parquet_to_csv(\"RESPONSible Service/dataset_polite.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_agreement(data_file: str, label_file: str) -> float:\n",
    "    # Load main CSV with human annotations\n",
    "    df_data = pd.read_csv(data_file, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "    # Load label CSV with model-assigned labels\n",
    "    df_labels = pd.read_csv(label_file, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "    # Ensure both files have the same number of rows\n",
    "    if len(df_data) != len(df_labels):\n",
    "        raise ValueError(\"Files do not match in number of rows.\")\n",
    "\n",
    "    # Extract human and label columns as integer lists\n",
    "    human = df_data[\"human\"].astype(str).str.strip()\n",
    "    label = df_labels[\"label\"].astype(str).str.strip()\n",
    "\n",
    "    # Filter out missing human annotations (empty strings or NaN)\n",
    "    valid_idx = (human != \"\") & (human != \"nan\")\n",
    "\n",
    "    human = human[valid_idx].astype(int).tolist()\n",
    "    label = label[valid_idx].astype(int).tolist()\n",
    "\n",
    "    if not human:\n",
    "        print(\"No human annotations found.\")\n",
    "        return 0.0\n",
    "\n",
    "    # Compute agreement\n",
    "    matches = sum(h == l for h, l in zip(human, label))\n",
    "    agreement_percent = matches / len(human) * 100\n",
    "\n",
    "    return round(agreement_percent, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE YOU RUN THIS CELL, MAKE SURE YOU HAVE COMPLETED THE FOLLOWING PASSAGES:\n",
    "    # 1. EXPORT THE dataset_{style}.csv FILES\n",
    "    # 2. MANUALLY ADD THE HUMAN LABELS\n",
    "    # 3. REPLACE THE dataset_{style}.csv FILES WITH THE MANUALLY ANNOTATED ONES (KEEP THE SAME FILE NAME)\n",
    "\n",
    "subsets = [\"clear\", 'friendly', \"empathetic\", \"polite\"]\n",
    "results = {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None}\n",
    "\n",
    "for subset in subsets:\n",
    "    csv1 = f\"hum_eval_dataset/dataset_{subset}.csv\"\n",
    "    csv2 = f\"hum_eval_dataset/dataset_{subset}_labels.csv\"\n",
    "    agreement_score = compute_agreement(csv1, csv2)\n",
    "    results[subset] = agreement_score\n",
    "\n",
    "print(\"Human Ground Truth Agreement (%) for the RESPONSible Service Dataset\")\n",
    "print()\n",
    "for key,value in results.items():\n",
    "    print(f\"{key}: {value}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Data Profiling\n",
    "\n",
    "The following sections include all the scripts that were used to compile the linguistic data profiling of the REPSONSible Service dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements_nlp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from transformers import pipeline\n",
    "import re\n",
    "from scipy.stats import shapiro, f_oneway, kruskal, ttest_ind, ranksums, chi2_contingency\n",
    "from itertools import combinations, zip_longest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import scikit_posthocs as sp\n",
    "import textstat\n",
    "from bert_score import score as bertscore\n",
    "from rouge_score import rouge_scorer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "path_clear = \"RESPONSible Service/dataset_clear.parquet\"\n",
    "path_friendly = \"RESPONSible Service/dataset_friendly.parquet\"\n",
    "path_empathetic = \"RESPONSible Service/dataset_empathetic.parquet\"\n",
    "path_polite = \"RESPONSible Service/dataset_polite.parquet\"\n",
    "\n",
    "df_clear = pd.read_parquet(path_clear)\n",
    "df_friendly = pd.read_parquet(path_friendly)\n",
    "df_empathetic = pd.read_parquet(path_empathetic)\n",
    "df_polite = pd.read_parquet(path_polite)\n",
    "\n",
    "req_c = df_clear['request'].tolist()\n",
    "req_f = df_friendly['request'].tolist()\n",
    "req_e = df_empathetic['request'].tolist()\n",
    "req_p = df_polite['request'].tolist()\n",
    "\n",
    "res_c = df_clear['response1'].tolist()\n",
    "res_f = df_friendly['response1'].tolist()\n",
    "res_e = df_empathetic['response1'].tolist()\n",
    "res_p = df_polite['response1'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NGSL frequency lists and concreteness scores for lexical complexity and lexical concreteness analyses\n",
    "\n",
    "with open(\"NGSL_1000.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ngsl_1000_txt = f.read()\n",
    "\n",
    "with open(\"NGSL_2000.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ngsl_2000_txt = f.read()\n",
    "\n",
    "with open(\"concreteness_scores.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    concreteness_txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM FUNCTION TO MAKE TEST STATISTICS RESULTS VISUALLY INTERPRETABLE\n",
    "\n",
    "def summarize_significant_differences_annotated(posthoc_matrix, styles):\n",
    "    # Create a mapping from full style names to single-letter initials\n",
    "    # e.g., {'clear': 'c', 'friendly': 'f', 'empathetic': 'e', 'polite': 'p'}\n",
    "    initials = {s: s[0] for s in styles}\n",
    "\n",
    "    summary = {}  # Will hold the final significance summary per style\n",
    "\n",
    "    # Iterate over each style (as the row label)\n",
    "    for i in styles:\n",
    "        sig_letters = []  # Track which styles differ significantly from 'i'\n",
    "\n",
    "        # Compare with all other styles (columns in the matrix)\n",
    "        for j in styles:\n",
    "            if i != j and posthoc_matrix.loc[i, j] < 0.05:\n",
    "                sig_letters.append(initials[j])  # Add significant ones\n",
    "\n",
    "        # Build the summary label:\n",
    "        # '*' if significantly different from all others\n",
    "        # '-' if not significantly different from any\n",
    "        # Else: list of initials of significantly different styles\n",
    "        if len(sig_letters) == len(styles) - 1:\n",
    "            summary[i] = '*'\n",
    "        elif len(sig_letters) == 0:\n",
    "            summary[i] = '-'\n",
    "        else:\n",
    "            summary[i] = ''.join(sorted(sig_letters))\n",
    "\n",
    "    return summary  # Dict: {style_name: significance_label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'w') as file:\n",
    "    print(\"=== FREQUENCY LISTS ===\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TOKEN FREQUENCY ANALYSIS WITH RATIOS ===\n",
    "\n",
    "# Function to count lemmatized, non-stopword, alphabetic tokens in a list of responses\n",
    "def get_all_token_counts(responses, nlp):\n",
    "    token_counts = Counter()\n",
    "    total_tokens = 0\n",
    "    for doc in nlp.pipe(responses):  # Efficient batch processing with spaCy\n",
    "        for token in doc:\n",
    "            if not token.is_stop and token.is_alpha:  # Filter out stopwords and non-alphabetic tokens\n",
    "                lemma = token.lemma_.lower()\n",
    "                token_counts[lemma] += 1\n",
    "                total_tokens += 1\n",
    "    return token_counts, total_tokens\n",
    "\n",
    "# Apply token counting to each set of responses (e.g., for different tones)\n",
    "counts_c, total_c = get_all_token_counts(res_c, nlp)  # \"Clear\"\n",
    "counts_f, total_f = get_all_token_counts(res_f, nlp)  # \"Friendly\"\n",
    "counts_e, total_e = get_all_token_counts(res_e, nlp)  # \"Empathetic\"\n",
    "counts_p, total_p = get_all_token_counts(res_p, nlp)  # \"Polite\"\n",
    "\n",
    "# Get the 20 most frequent tokens per tone\n",
    "freq_clear = counts_c.most_common(20)\n",
    "freq_friendly = counts_f.most_common(20)\n",
    "freq_empathetic = counts_e.most_common(20)\n",
    "freq_polite = counts_p.most_common(20)\n",
    "\n",
    "# Store all token counts and total tokens for statistical comparison\n",
    "all_counts = {\n",
    "    \"c\": (counts_c, total_c),\n",
    "    \"f\": (counts_f, total_f),\n",
    "    \"e\": (counts_e, total_e),\n",
    "    \"p\": (counts_p, total_p)\n",
    "}\n",
    "\n",
    "# Perform a chi-square test for each token's frequency in one subset vs. the others\n",
    "def get_significance_label(token, initial):\n",
    "    sig_against = []  # Track which other subsets the token is significantly different from\n",
    "    all_subsets = {\"c\", \"f\", \"e\", \"p\"} - {initial}\n",
    "    tok1, total1 = all_counts[initial]\n",
    "    tok1_count = tok1.get(token, 0)\n",
    "    not_tok1 = total1 - tok1_count\n",
    "\n",
    "    for other in all_subsets:\n",
    "        tok2, total2 = all_counts[other]\n",
    "        tok2_count = tok2.get(token, 0)\n",
    "        not_tok2 = total2 - tok2_count\n",
    "\n",
    "        # Create a 2x2 contingency table for chi-square test\n",
    "        contingency = [[tok1_count, not_tok1], [tok2_count, not_tok2]]\n",
    "        chi2, p, _, _ = chi2_contingency(contingency)\n",
    "        if p < 0.05:  # Statistically significant\n",
    "            sig_against.append(other)\n",
    "\n",
    "    # Return a label indicating significance\n",
    "    if set(sig_against) == all_subsets:\n",
    "        return \"*\"  # Significant against all other groups\n",
    "    elif sig_against:\n",
    "        return ''.join(sorted(sig_against))  # Significant against specific subsets\n",
    "    else:\n",
    "        return \"-\"  # Not significant\n",
    "\n",
    "# Gather all unique top-20 tokens across subsets for later reference (optional use)\n",
    "top_tokens = set([w for w, _ in freq_clear] + [w for w, _ in freq_friendly] +\n",
    "                 [w for w, _ in freq_empathetic] + [w for w, _ in freq_polite])\n",
    "\n",
    "# Write formatted results to a text file\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nToken Frequency Analysis (Top 20 Lemmas)\\n\", file=file)\n",
    "\n",
    "    # Header row: Rank, token, frequency (%) and significance score (SS) for each subset\n",
    "    headers = [\"Rank\", \"Clear\", \"Freq (%)\", \"SS\", \"Friendly\", \"Freq (%)\", \"SS\",\n",
    "               \"Empath.\", \"Freq (%)\", \"SS\", \"Polite\", \"Freq (%)\", \"SS\"]\n",
    "    print(f\"{headers[0]:<5} {headers[1]:<13} {headers[2]:<10} {headers[3]:<3} \"\n",
    "          f\"{headers[4]:<13} {headers[5]:<10} {headers[6]:<3} \"\n",
    "          f\"{headers[7]:<13} {headers[8]:<10} {headers[9]:<3} \"\n",
    "          f\"{headers[10]:<13} {headers[11]:<10} {headers[12]:<3}\", file=file)\n",
    "    print(\"-\" * 125, file=file)\n",
    "\n",
    "    # Write each row of results for the top 20 tokens\n",
    "    for i in range(20):\n",
    "        row = []\n",
    "        for label, freq_list, total, count_dict in zip(\n",
    "            ['c', 'f', 'e', 'p'],\n",
    "            [freq_clear, freq_friendly, freq_empathetic, freq_polite],\n",
    "            [total_c, total_f, total_e, total_p],\n",
    "            [counts_c, counts_f, counts_e, counts_p]\n",
    "        ):\n",
    "            word, freq = freq_list[i] if i < len(freq_list) else (\"\", 0)\n",
    "            freq_pct = f\"{(freq / total * 100):.1f}%\" if total > 0 else \"0.0%\"\n",
    "            sig_label = get_significance_label(word, label) if word else \"\"\n",
    "            row.extend([word, f\"{freq} ({freq_pct})\", sig_label])\n",
    "\n",
    "        # Print the formatted row\n",
    "        print(f\"{i+1:<5} \"\n",
    "              f\"{row[0]:<13} {row[1]:<10} {row[2]:<3} \"\n",
    "              f\"{row[3]:<13} {row[4]:<10} {row[5]:<3} \"\n",
    "              f\"{row[6]:<13} {row[7]:<10} {row[8]:<3} \"\n",
    "              f\"{row[9]:<13} {row[10]:<10} {row[11]:<3}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === POS FREQUENCY ANALYSIS ===\n",
    "\n",
    "# Function to compute POS tag frequencies for a list of responses\n",
    "def get_ranked_pos(responses):\n",
    "    pos_freq = Counter()\n",
    "    for doc in nlp.pipe(responses):  # Efficiently process with spaCy's pipeline\n",
    "        for token in doc:\n",
    "            if not token.is_space:  # Exclude spaces\n",
    "                pos_freq[token.pos_] += 1  # Count the coarse-grained POS tag\n",
    "    total = sum(pos_freq.values()) or 1  # Avoid division by zero\n",
    "    # Return a list of (POS tag, raw count, percentage)\n",
    "    return [(tag, count, count / total * 100) for tag, count in pos_freq.most_common()]\n",
    "\n",
    "# === Compute POS tag frequencies for each tone-specific response set ===\n",
    "clear = get_ranked_pos(res_c)        # \"Clear\" tone\n",
    "friendly = get_ranked_pos(res_f)     # \"Friendly\" tone\n",
    "empathetic = get_ranked_pos(res_e)   # \"Empathetic\" tone\n",
    "polite = get_ranked_pos(res_p)       # \"Polite\" tone\n",
    "\n",
    "# === Gather all unique POS tags across all subsets ===\n",
    "all_tags = sorted(set(tag for data in [clear, friendly, empathetic, polite] for tag, _, _ in data))\n",
    "\n",
    "# Helper: Convert list of tuples into a lookup dictionary {POS tag: count}\n",
    "def build_lookup(data):\n",
    "    return {tag: count for tag, count, _ in data}\n",
    "\n",
    "# Build lookup dicts for quick access\n",
    "clear_counts = build_lookup(clear)\n",
    "friendly_counts = build_lookup(friendly)\n",
    "empathetic_counts = build_lookup(empathetic)\n",
    "polite_counts = build_lookup(polite)\n",
    "\n",
    "# Total token counts per subset (used for chi-square)\n",
    "total_counts = {\n",
    "    \"c\": sum(clear_counts.values()),\n",
    "    \"f\": sum(friendly_counts.values()),\n",
    "    \"e\": sum(empathetic_counts.values()),\n",
    "    \"p\": sum(polite_counts.values())\n",
    "}\n",
    "\n",
    "# POS tag lookup per subset\n",
    "tag_lookup = {\n",
    "    \"c\": clear_counts,\n",
    "    \"f\": friendly_counts,\n",
    "    \"e\": empathetic_counts,\n",
    "    \"p\": polite_counts,\n",
    "}\n",
    "\n",
    "# === Chi-square significance testing across subsets ===\n",
    "def get_significance_chi2(tag):\n",
    "    labels = [\"c\", \"f\", \"e\", \"p\"]\n",
    "\n",
    "    # Get the count of the tag in each subset\n",
    "    counts = [tag_lookup[l].get(tag, 0) for l in labels]\n",
    "    # Complementary counts (everything but this tag)\n",
    "    comps = [total_counts[l] - counts[i] for i, l in enumerate(labels)]\n",
    "    table = [counts, comps]\n",
    "\n",
    "    # Overall chi-square test\n",
    "    chi2, pval, _, _ = chi2_contingency(table)\n",
    "    if pval >= 0.05:\n",
    "        return {l: \"-\" for l in labels}  # Not statistically significant overall\n",
    "\n",
    "    # If significant overall, run pairwise chi-square tests\n",
    "    sig_map = {l: [] for l in labels}\n",
    "    pvals = []\n",
    "    pairs = []\n",
    "\n",
    "    for a, b in combinations(labels, 2):\n",
    "        a_count = tag_lookup[a].get(tag, 0)\n",
    "        b_count = tag_lookup[b].get(tag, 0)\n",
    "        sub_table = [\n",
    "            [a_count, b_count],\n",
    "            [total_counts[a] - a_count, total_counts[b] - b_count]\n",
    "        ]\n",
    "        _, pval_pair, _, _ = chi2_contingency(sub_table)\n",
    "        pvals.append(pval_pair)\n",
    "        pairs.append((a, b))\n",
    "\n",
    "    # Apply Bonferroni correction for multiple comparisons\n",
    "    reject, _, _, _ = multipletests(pvals, alpha=0.05, method='bonferroni')\n",
    "    for (a, b), r in zip(pairs, reject):\n",
    "        if r:  # If null hypothesis is rejected, mark as significantly different\n",
    "            sig_map[a].append(b)\n",
    "            sig_map[b].append(a)\n",
    "\n",
    "    # Format significance result per subset\n",
    "    result = {}\n",
    "    for l in labels:\n",
    "        if len(sig_map[l]) == 3:\n",
    "            result[l] = \"*\"  # Significantly different from all others\n",
    "        elif not sig_map[l]:\n",
    "            result[l] = \"-\"  # No significant difference\n",
    "        else:\n",
    "            result[l] = ''.join(sorted(sig_map[l]))  # List of significantly different subsets\n",
    "    return result\n",
    "\n",
    "# === Attach significance labels to each POS tag record ===\n",
    "def enrich_with_ss(data, subset_key):\n",
    "    enriched = []\n",
    "    for tag, count, pct in data:\n",
    "        ss = get_significance_chi2(tag)[subset_key]  # Get significance label for this subset\n",
    "        enriched.append((tag, count, pct, ss))\n",
    "    return enriched\n",
    "\n",
    "# Enrich each subset's data with significance annotations\n",
    "clear_enriched = enrich_with_ss(clear, \"c\")\n",
    "friendly_enriched = enrich_with_ss(friendly, \"f\")\n",
    "empathetic_enriched = enrich_with_ss(empathetic, \"e\")\n",
    "polite_enriched = enrich_with_ss(polite, \"p\")\n",
    "\n",
    "# === Determine number of rows needed (longest list) ===\n",
    "TOP_N = max(len(clear_enriched), len(friendly_enriched), len(empathetic_enriched), len(polite_enriched))\n",
    "\n",
    "# Pad each list to the same length to ensure row alignment in the output\n",
    "def pad(data, n):\n",
    "    return data + [(\"\", 0, 0.0, \"\")] * (n - len(data))\n",
    "\n",
    "clear_enriched = pad(clear_enriched, TOP_N)\n",
    "friendly_enriched = pad(friendly_enriched, TOP_N)\n",
    "empathetic_enriched = pad(empathetic_enriched, TOP_N)\n",
    "polite_enriched = pad(polite_enriched, TOP_N)\n",
    "\n",
    "# === Write formatted output to a text file ===\n",
    "with open(\"results.txt\", \"a\") as file:\n",
    "    header = (\n",
    "        f\"{'Rank':<5} \"\n",
    "        f\"{'Clear':<10} {'Count':<7} {'%':<6} {'SS':<3}   \"\n",
    "        f\"{'Friendly':<10} {'Count':<7} {'%':<6} {'SS':<3}   \"\n",
    "        f\"{'Empathic':<10} {'Count':<7} {'%':<6} {'SS':<3}   \"\n",
    "        f\"{'Polite':<10} {'Count':<7} {'%':<6} {'SS':<3}\"\n",
    "    )\n",
    "    print(\"POS Frequency Analysis\\n\", file=file)\n",
    "    print(header, file=file)\n",
    "    print(\"-\" * len(header), file=file)\n",
    "\n",
    "    # Print each row of POS tag data across all subsets\n",
    "    for i in range(TOP_N):\n",
    "        c_tag, c_count, c_pct, c_ss = clear_enriched[i]\n",
    "        f_tag, f_count, f_pct, f_ss = friendly_enriched[i]\n",
    "        e_tag, e_count, e_pct, e_ss = empathetic_enriched[i]\n",
    "        p_tag, p_count, p_pct, p_ss = polite_enriched[i]\n",
    "\n",
    "        line = (\n",
    "            f\"{i+1:<5} \"\n",
    "            f\"{c_tag:<10} {c_count:<7} {c_pct:<6.1f} {c_ss:<3}   \"\n",
    "            f\"{f_tag:<10} {f_count:<7} {f_pct:<6.1f} {f_ss:<3}   \"\n",
    "            f\"{e_tag:<10} {e_count:<7} {e_pct:<6.1f} {e_ss:<3}   \"\n",
    "            f\"{p_tag:<10} {p_count:<7} {p_pct:<6.1f} {p_ss:<3}\"\n",
    "        )\n",
    "        print(line, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TOKEN FREQUENCY ANALYSIS (PUNCTUATION) ===\n",
    "\n",
    "# Function to count punctuation tokens across a set of responses\n",
    "def get_all_token_counts(responses, nlp):\n",
    "    token_counts = Counter()\n",
    "    total_tokens = 0\n",
    "    for doc in nlp.pipe(responses):  # Efficient batch processing\n",
    "        for token in doc:\n",
    "            if token.is_punct:  # Only include punctuation tokens\n",
    "                token_counts[token.text] += 1\n",
    "                total_tokens += 1\n",
    "    return token_counts, total_tokens\n",
    "\n",
    "# Count punctuation tokens in each subset\n",
    "counts_c, total_c = get_all_token_counts(res_c, nlp)  # Clear tone\n",
    "counts_f, total_f = get_all_token_counts(res_f, nlp)  # Friendly tone\n",
    "counts_e, total_e = get_all_token_counts(res_e, nlp)  # Empathetic tone\n",
    "counts_p, total_p = get_all_token_counts(res_p, nlp)  # Polite tone\n",
    "\n",
    "# Sort punctuation tokens by relative frequency (percentage), descending\n",
    "def sort_by_relative_freq(counts, total):\n",
    "    return sorted(counts.items(), key=lambda x: x[1] / total, reverse=True)\n",
    "\n",
    "# Top punctuation tokens for each subset\n",
    "top_c = sort_by_relative_freq(counts_c, total_c)\n",
    "top_f = sort_by_relative_freq(counts_f, total_f)\n",
    "top_e = sort_by_relative_freq(counts_e, total_e)\n",
    "top_p = sort_by_relative_freq(counts_p, total_p)\n",
    "\n",
    "# Bundle token counts and totals for access in significance testing\n",
    "all_counts = {\n",
    "    \"c\": (counts_c, total_c),\n",
    "    \"f\": (counts_f, total_f),\n",
    "    \"e\": (counts_e, total_e),\n",
    "    \"p\": (counts_p, total_p)\n",
    "}\n",
    "\n",
    "# Chi-square significance testing: is this token used differently in one subset vs others?\n",
    "def get_significance_label(token, initial):\n",
    "    sig_against = []\n",
    "    all_subsets = {\"c\", \"f\", \"e\", \"p\"} - {initial}\n",
    "    tok1, total1 = all_counts[initial]\n",
    "    tok1_count = tok1.get(token, 0)\n",
    "    not_tok1 = total1 - tok1_count\n",
    "\n",
    "    for other in all_subsets:\n",
    "        tok2, total2 = all_counts[other]\n",
    "        tok2_count = tok2.get(token, 0)\n",
    "        not_tok2 = total2 - tok2_count\n",
    "\n",
    "        # 2x2 contingency table for chi-square test\n",
    "        contingency = [[tok1_count, not_tok1], [tok2_count, not_tok2]]\n",
    "        chi2, p, _, _ = chi2_contingency(contingency)\n",
    "        if p < 0.05:\n",
    "            sig_against.append(other)\n",
    "\n",
    "    # Label formatting\n",
    "    if set(sig_against) == all_subsets:\n",
    "        return \"*\"  # Significant against all others\n",
    "    elif sig_against:\n",
    "        return ''.join(sorted(sig_against))  # Significant against some\n",
    "    else:\n",
    "        return \"-\"  # Not significant\n",
    "\n",
    "# === Formatting and Output ===\n",
    "\n",
    "# Define column widths for alignment\n",
    "W_TOKEN = 8  # Token text width\n",
    "W_ABS = 6    # Raw frequency width\n",
    "W_REL = 7    # Relative frequency (percentage)\n",
    "W_SS = 4     # Significance score\n",
    "\n",
    "# Format one block of a token's data (text, count, percent, significance)\n",
    "def format_block(token, freq, total, label):\n",
    "    if not token:\n",
    "        return \" \" * (W_TOKEN + W_ABS + W_REL + W_SS)  # Empty row if no token\n",
    "    rel = f\"{(freq / total * 100):.1f}%\" if total > 0 else \"0.0%\"  # Compute %\n",
    "    sig = get_significance_label(token, label)  # Get significance label\n",
    "    return f\"{token:<{W_TOKEN}}{freq:>{W_ABS}}{rel:>{W_REL}}{sig:>{W_SS}}\"\n",
    "\n",
    "# === Write the formatted output to a results file ===\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nToken Frequency Analysis (Punctuation)\\n\", file=file)\n",
    "\n",
    "    # Header with subset names\n",
    "    header_names = ['Clear', 'Friendly', 'Empath.', 'Polite']\n",
    "    header_line = \"     \" + \"   \".join(\n",
    "        f\"{name:<{W_TOKEN + W_ABS + W_REL + W_SS}}\" for name in header_names\n",
    "    )\n",
    "\n",
    "    # Subheader with column labels\n",
    "    subheader_line = f\"{'#':<4}\" + \"   \".join(\n",
    "        f\"{'Token':<{W_TOKEN}}{'Freq':>{W_ABS}}{'(%)':>{W_REL}}{'SS':>{W_SS}}\" for _ in header_names\n",
    "    )\n",
    "\n",
    "    print(header_line, file=file)\n",
    "    print(subheader_line, file=file)\n",
    "    print(\"-\" * len(subheader_line), file=file)\n",
    "\n",
    "    # For each row, print data for all subsets (default to blank if no token)\n",
    "    for i, rows in enumerate(zip_longest(top_c, top_f, top_e, top_p, fillvalue=(\"\", 0))):\n",
    "        row = f\"{i+1:<4}\"\n",
    "        for (token, freq), label in zip(rows, ['c', 'f', 'e', 'p']):\n",
    "            total = all_counts[label][1]\n",
    "            row += format_block(token, freq, total, label) + \"   \"\n",
    "        print(row.rstrip(), file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'a') as file:\n",
    "    print(\"=== LEXICAL FEATURES ===\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LEXICAL DIVERSITY ANALYSIS USING TOKEN-TYPE RATIO (TTR) ===\n",
    "\n",
    "# TTR calculation\n",
    "def compute_ttr_value(texts, nlp):\n",
    "    def calculate_ttr(text):\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_.lower() for token in doc if token.is_alpha]\n",
    "        if not tokens:\n",
    "            return None\n",
    "        return len(set(tokens)) / len(tokens)\n",
    "\n",
    "    ttr_values = [calculate_ttr(text) for text in texts]\n",
    "    ttr_values = [val for val in ttr_values if val is not None]\n",
    "    avg_ttr = sum(ttr_values) / len(ttr_values) if ttr_values else 0\n",
    "    return avg_ttr, ttr_values\n",
    "\n",
    "# Input styles and responses\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "requests = [req_c, req_f, req_e, req_p]\n",
    "responses = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "ttr_report = {style: {} for style in styles}\n",
    "ttr_scores = {}\n",
    "\n",
    "# Compute TTR for each style\n",
    "for style, res in zip(styles, responses):\n",
    "    avg_ttr, ttr_vals = compute_ttr_value(res, nlp=nlp)\n",
    "    ttr_report[style][\"avg_ttr\"] = avg_ttr\n",
    "    ttr_scores[style] = ttr_vals\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "shapiro_results = {style: shapiro(vals) for style, vals in ttr_scores.items()}\n",
    "all_normal = all(result.pvalue >= 0.05 for result in shapiro_results.values())\n",
    "\n",
    "# Statistical test\n",
    "if all_normal:\n",
    "    stat_test = \"ANOVA\"\n",
    "    stat, p_val = f_oneway(*[ttr_scores[style] for style in styles])\n",
    "else:\n",
    "    stat_test = \"Kruskal-Wallis\"\n",
    "    stat, p_val = kruskal(*[ttr_scores[style] for style in styles])\n",
    "\n",
    "# Post-hoc test if significant\n",
    "posthoc = None\n",
    "if p_val < 0.05:\n",
    "    all_scores = [val for vals in ttr_scores.values() for val in vals]\n",
    "    all_labels = [style for style in styles for _ in ttr_scores[style]]\n",
    "    df_posthoc = pd.DataFrame({\"score\": all_scores, \"style\": all_labels})\n",
    "    posthoc = sp.posthoc_dunn(df_posthoc, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "# Output results\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nLexical Diversity (TTR, average)\", file=file)\n",
    "    print(f\"\\n\\tStatistical Test: {stat_test} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    if posthoc is not None:\n",
    "        sig_annotated = summarize_significant_differences_annotated(posthoc, styles)\n",
    "        for style in styles:\n",
    "            mean_val = ttr_report[style][\"avg_ttr\"]\n",
    "            sig_diff = sig_annotated[style]\n",
    "            print(f\"\\t{style:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        for style in styles:\n",
    "            mean_val = ttr_report[style][\"avg_ttr\"]\n",
    "            print(f\"\\t{style:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LEXICAL COMPLEXITY ANALYSIS WITH STATISTICAL SIGNIFICANCE ===\n",
    "\n",
    "def compute_ngsl_distribution_per_text(responses, ngsl_1000_txt, ngsl_2000_txt, nlp):\n",
    "    # Compute the proportion of lemmatized content words in each response that appear in the NGSL (New General Service List) top 2000 vocabulary.\n",
    "    # This proportion serves as a proxy for lexical simplicity: higher = simpler.\n",
    "\n",
    "    def load_word_list(filename):\n",
    "        # Load word list from raw text (one word per line)\n",
    "        lines = filename.splitlines()\n",
    "        return set(word.strip().lower() for word in lines)\n",
    "\n",
    "    def tokenize_and_lemmatize(text):\n",
    "        # Tokenize and lemmatize a string using spaCy\n",
    "        doc = nlp(text)\n",
    "        return [\n",
    "            token.lemma_.lower()\n",
    "            for token in doc\n",
    "            if token.is_alpha and not token.is_punct and not token.is_stop\n",
    "        ]\n",
    "\n",
    "    # Load NGSL 1000 and 2000 lists\n",
    "    ngsl_1000 = load_word_list(ngsl_1000_txt)\n",
    "    ngsl_2000 = load_word_list(ngsl_2000_txt)\n",
    "\n",
    "    # Combine and lemmatize all NGSL words\n",
    "    ngsl_combined = set(tokenize_and_lemmatize(' '.join(ngsl_1000 | ngsl_2000)))\n",
    "\n",
    "    # Compute NGSL ratio per response\n",
    "    proportions = []\n",
    "    for doc in nlp.pipe(responses, batch_size=50):\n",
    "        tokens = [\n",
    "            token.lemma_.lower()\n",
    "            for token in doc\n",
    "            if token.is_alpha and not token.is_punct and not token.is_stop\n",
    "        ]\n",
    "        total = len(tokens)\n",
    "        if total == 0:\n",
    "            proportions.append(0)\n",
    "        else:\n",
    "            count_ngsl = sum(1 for t in tokens if t in ngsl_combined)\n",
    "            proportions.append(count_ngsl / total)\n",
    "\n",
    "    return proportions\n",
    "\n",
    "\n",
    "# === SETUP: Define tone groups and store results ===\n",
    "\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "requests = [req_c, req_f, req_e, req_p]      # (Not used in this script)\n",
    "responses = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Containers for output\n",
    "ngsl_report = {style: {} for style in styles}  # To store average NGSL ratios\n",
    "ngsl_scores = {}  # To store per-response NGSL ratios\n",
    "\n",
    "# === COMPUTE: Get NGSL-based lexical simplicity scores ===\n",
    "\n",
    "for style, res in zip(styles, responses):\n",
    "    ratios = compute_ngsl_distribution_per_text(\n",
    "        res,\n",
    "        ngsl_1000_txt,\n",
    "        ngsl_2000_txt,\n",
    "        nlp=nlp\n",
    "    )\n",
    "    ngsl_report[style][\"ngsl_ratio\"] = np.mean(ratios)\n",
    "    ngsl_scores[style] = ratios\n",
    "\n",
    "# === STATISTICAL TESTING AND OUTPUT ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nLexical Complexity (NGSL frequency list, average)\", file=file)\n",
    "\n",
    "    # 1. Shapiro-Wilk normality test\n",
    "    shapiro_p = {s: shapiro(ngsl_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "\n",
    "    # 2. Choose appropriate test\n",
    "    data = [ngsl_scores[s] for s in styles]\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # 3. Posthoc testing if significant\n",
    "    if p_val < 0.05:\n",
    "        # Prepare flat list of all values and labels\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s]*len(ngsl_scores[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Run Dunn's posthoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Print results with annotations\n",
    "        for s in styles:\n",
    "            mean_val = ngsl_report[s][\"ngsl_ratio\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.2f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If no significant difference, print averages only\n",
    "        for s in styles:\n",
    "            mean_val = ngsl_report[s][\"ngsl_ratio\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.2f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LEXICAL REPETITION ANALYSIS ===\n",
    "\n",
    "# Function to compute repetition scores per text\n",
    "def compute_lexical_repetition_per_text(texts, nlp):\n",
    "    # For each text, computes a repetition score by sliding windows of sizes 2, 5, and 10, and counting repeated unigrams, bigrams, and trigrams within those windows.\n",
    "    # The final score is the sum of all repetition counts across all windows.\n",
    "\n",
    "    # Helper to extract n-grams of size n\n",
    "    def get_ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "    # Count how many repeated n-grams exist within a window\n",
    "    def count_lexical_repetitions(tokens, window_size):\n",
    "        total_repetitions = 0\n",
    "        for i in range(len(tokens) - window_size + 1):\n",
    "            window = tokens[i:i+window_size]\n",
    "            window_ngrams = []\n",
    "            for n in [1, 2, 3]:  # Unigrams, bigrams, trigrams\n",
    "                ngrams = get_ngrams(window, n)\n",
    "                window_ngrams.extend(ngrams)\n",
    "            counts = Counter(window_ngrams)\n",
    "            repetitions = sum(count - 1 for count in counts.values() if count > 1)\n",
    "            total_repetitions += repetitions\n",
    "        return total_repetitions\n",
    "\n",
    "    # Window sizes to scan for repetitions\n",
    "    window_sizes = [2, 5, 10]\n",
    "    repetition_scores = []\n",
    "\n",
    "    # Process each text\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text.lower() for token in doc if token.is_alpha]  # Keep only alphabetic words\n",
    "        score = sum(count_lexical_repetitions(tokens, w) for w in window_sizes)\n",
    "        repetition_scores.append(score)\n",
    "\n",
    "    return repetition_scores\n",
    "\n",
    "\n",
    "# === SETUP ===\n",
    "\n",
    "# Define style categories and collect corresponding responses\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "requests = [req_c, req_f, req_e, req_p]     # (Unused in this script)\n",
    "responses = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Containers for storing results\n",
    "repetition_report = {style: {} for style in styles}\n",
    "repetition_scores = {}\n",
    "\n",
    "# Compute repetition scores for each style\n",
    "for style, res in zip(styles, responses):\n",
    "    scores = compute_lexical_repetition_per_text(res, nlp=nlp)\n",
    "    repetition_report[style][\"lexical_repetition\"] = np.mean(scores)  # Average score per style\n",
    "    repetition_scores[style] = scores  # Individual scores\n",
    "\n",
    "# === OUTPUT: REPORT TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nLexical Repetition (across n-grams, average)\", file=file)\n",
    "\n",
    "    # Step 1: Test for normality using Shapiro-Wilk\n",
    "    shapiro_p = {s: shapiro(repetition_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "\n",
    "    # Step 2: Choose statistical test based on normality\n",
    "    data = [repetition_scores[s] for s in styles]\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # Step 3: If significant, run posthoc test\n",
    "    if p_val < 0.05:\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s] * len(repetition_scores[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Dunn's posthoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Print results with annotations\n",
    "        for s in styles:\n",
    "            mean_val = repetition_report[s][\"lexical_repetition\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.2f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If not significant, print just the averages\n",
    "        for s in styles:\n",
    "            mean_val = repetition_report[s][\"lexical_repetition\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.2f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INFORMATION DENSITY ANALYSIS WITH STATISTICAL SIGNIFICANCE ===\n",
    "\n",
    "# Function to compute information density per text\n",
    "def compute_info_density_per_text(texts, nlp):\n",
    "    # Computes information density for each text:\n",
    "    # Defined as the proportion of content words (nouns, verbs, adjectives, adverbs) relative to the total number of non-punctuation, non-space tokens.\n",
    "    \n",
    "    densities = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        # Content words = semantic \"load-bearers\"\n",
    "        content_words = [token for token in doc if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]]\n",
    "        # Total words = all tokens except spaces and punctuation\n",
    "        total_words = [token for token in doc if not token.is_space and not token.is_punct]\n",
    "        # Compute density\n",
    "        density = len(content_words) / len(total_words) if total_words else 0\n",
    "        densities.append(density)\n",
    "    return densities\n",
    "\n",
    "\n",
    "# === SETUP: Define tone categories and response sets ===\n",
    "\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "responses_list = [res_c, res_f, res_e, res_p]  # Response texts per tone\n",
    "\n",
    "# Containers to store results\n",
    "info_density_report = {style: {} for style in styles}  # Average density per style\n",
    "info_density_scores = {}  # All individual density scores per style\n",
    "\n",
    "\n",
    "# === COMPUTE INFORMATION DENSITY SCORES ===\n",
    "\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    densities = compute_info_density_per_text(resps, nlp=nlp)\n",
    "    info_density_report[style][\"information_density\"] = np.mean(densities)\n",
    "    info_density_scores[style] = densities\n",
    "\n",
    "# === STATISTICAL TESTING AND OUTPUT TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nInformation density (content words / total words, average)\", file=file)\n",
    "\n",
    "    # 1. Normality Test (Shapiro-Wilk)\n",
    "    shapiro_p = {s: shapiro(info_density_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "\n",
    "    # 2. Choose statistical test based on normality\n",
    "    data = [info_density_scores[s] for s in styles]\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # 3. If significant, run posthoc test\n",
    "    if p_val < 0.05:\n",
    "        # Flatten scores and build labeled DataFrame\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s]*len(info_density_scores[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Perform Dunn's posthoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Summarize which groups differ significantly\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Output average scores with significance labels\n",
    "        for s in styles:\n",
    "            mean_val = info_density_report[s][\"information_density\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If no significant differences, just print average scores\n",
    "        for s in styles:\n",
    "            mean_val = info_density_report[s][\"information_density\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEXT LENGTH ANALYSIS ===\n",
    "\n",
    "# Function to compute token length per response\n",
    "def compute_token_lengths_per_text(responses, nlp):\n",
    "\n",
    "    # Takes a list of text responses and returns a list of token counts\n",
    "    # (i.e., number of tokens in each response), using spaCy's tokenizer.\n",
    "\n",
    "    return [len(nlp(text)) for text in responses]\n",
    "\n",
    "\n",
    "# === SETUP ===\n",
    "\n",
    "# Define styles and corresponding responses\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Containers to store average and individual token lengths\n",
    "token_length_report = {style: {} for style in styles}\n",
    "token_length_scores = {}\n",
    "\n",
    "\n",
    "# === COMPUTE TOKEN LENGTHS ===\n",
    "\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    lengths = compute_token_lengths_per_text(resps, nlp=nlp)\n",
    "    token_length_report[style][\"token_length\"] = np.mean(lengths)\n",
    "    token_length_scores[style] = lengths\n",
    "\n",
    "# === STATISTICAL TESTING & OUTPUT ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nSentence length (average)\", file=file)\n",
    "\n",
    "    # Step 1: Test for normality using Shapiro-Wilk\n",
    "    shapiro_p = {s: shapiro(token_length_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "\n",
    "    # Step 2: Choose statistical test\n",
    "    data = [token_length_scores[s] for s in styles]\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # Step 3: If significant, perform posthoc test\n",
    "    if p_val < 0.05:\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s] * len(token_length_scores[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Perform Dunn's posthoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Summarize significance results\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Output mean values and annotations\n",
    "        for s in styles:\n",
    "            mean_val = token_length_report[s][\"token_length\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.2f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If not significant, output only mean values\n",
    "        for s in styles:\n",
    "            mean_val = token_length_report[s][\"token_length\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.2f}\", file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discourse Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\n=== DISCOURSE FEATURES ===\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COHESION ANALYSIS ===\n",
    "\n",
    "# Function to compute the proportion of cohesive devices in each text\n",
    "def compute_total_cohesion_per_text(texts, nlp):\n",
    "    # List of common cohesive devices (connectives, transitions, etc.)\n",
    "    cohesive_devices = [\n",
    "        \"and\", \"also\", \"too\", \"as well\", \"in addition\", \"furthermore\", \"moreover\",\n",
    "        \"besides\", \"what's more\", \"for example\", \"for instance\", \"such as\", \"like\",\n",
    "        \"including\", \"namely\", \"in particular\", \"similarly\", \"likewise\", \"in the same way\",\n",
    "        \"equally\", \"just as\", \"in other words\", \"that is to say\", \"i.e.\", \"to put it another way\",\n",
    "        \"because\", \"since\", \"as\", \"due to\", \"owing to\", \"for this reason\", \"so\", \"therefore\",\n",
    "        \"thus\", \"consequently\", \"as a result\", \"hence\", \"accordingly\", \"in order to\", \"so that\",\n",
    "        \"for the purpose of\", \"with the aim of\", \"but\", \"however\", \"although\", \"even though\",\n",
    "        \"whereas\", \"while\", \"on the other hand\", \"in contrast\", \"yet\", \"nonetheless\",\n",
    "        \"nevertheless\", \"still\", \"admittedly\", \"of course\", \"even so\", \"while it is true that\",\n",
    "        \"or\", \"alternatively\", \"on the one hand\", \"on the other hand\", \"either\", \"or\",\n",
    "        \"neither\", \"nor\", \"then\", \"next\", \"after that\", \"subsequently\", \"eventually\",\n",
    "        \"finally\", \"at last\", \"while\", \"as\", \"at the same time\", \"meanwhile\", \"during\",\n",
    "        \"before\", \"previously\", \"earlier\", \"up to that point\", \"until then\", \"after\",\n",
    "        \"later\", \"afterwards\", \"since then\", \"now\", \"at present\", \"currently\", \"at that moment\",\n",
    "        \"by then\", \"at that time\", \"in conclusion\", \"in summary\", \"to sum up\", \"overall\",\n",
    "        \"eventually\"\n",
    "    ]\n",
    "\n",
    "    # Create a set of lowercase cohesive devices for quick lookup\n",
    "    cohesive_set = set(word.lower() for word in cohesive_devices)\n",
    "\n",
    "    scores = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)  # Process the text with spaCy NLP pipeline\n",
    "        # Count tokens that are not punctuation or whitespace\n",
    "        token_count = len([t for t in doc if not t.is_punct and not t.is_space])\n",
    "        # Count how many tokens match cohesive devices\n",
    "        match_count = sum(1 for token in doc if token.text.lower() in cohesive_set)\n",
    "        # Compute the ratio of cohesive devices to total tokens\n",
    "        score = match_count / token_count if token_count else 0.0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# === PREPARATION ===\n",
    "\n",
    "# Define the four styles\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "# Corresponding responses for each style\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Initialize a report dictionary to hold average cohesion per style\n",
    "total_cohesion_report = {style: {} for style in styles}\n",
    "# Dictionary to hold individual cohesion scores per response\n",
    "total_cohesion_scores = {}\n",
    "\n",
    "# === COMPUTE COHESION SCORES ===\n",
    "\n",
    "# Compute cohesion scores for each style's responses\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    scores = compute_total_cohesion_per_text(resps, nlp)\n",
    "    total_cohesion_report[style][\"total_cohesion\"] = np.mean(scores)  # Store average\n",
    "    total_cohesion_scores[style] = scores  # Store all individual scores\n",
    "\n",
    "# === REPORTING AND STATISTICAL TESTING ===\n",
    "\n",
    "# Append results to output file\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nCohesion (frequency of connectives, average)\", file=file)\n",
    "\n",
    "    # Collect all scores for statistical testing\n",
    "    data = [total_cohesion_scores[s] for s in styles]\n",
    "\n",
    "    # Perform Shapiro-Wilk test for normality on each group\n",
    "    shapiro_p = {s: shapiro(total_cohesion_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "\n",
    "    # Choose parametric or non-parametric test based on normality\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # If significant differences exist, perform post-hoc testing\n",
    "    if p_val < 0.05:\n",
    "        # Flatten values and create labels for post-hoc analysis\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s]*len(total_cohesion_scores[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Dunn's post-hoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Annotate significance results\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Print results with significance markers\n",
    "        for s in styles:\n",
    "            mean_val = total_cohesion_report[s][\"total_cohesion\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If no significant differences, just print means\n",
    "        for s in styles:\n",
    "            mean_val = total_cohesion_report[s][\"total_cohesion\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === REQUEST/RESPONSE SEMANTIC OVERLAP ANALYSIS (BERTScore) ===\n",
    "\n",
    "# Function to compute BERTScore F1 for each request-response pair\n",
    "def compute_bert_f1_scores(requests, responses):\n",
    "    # Calculate BERTScore using pre-trained 'bert-base-uncased' model\n",
    "    # Only the F1 scores are extracted here\n",
    "    _, _, F1 = bertscore(responses, requests, lang='en', model_type='bert-base-uncased', verbose=True)\n",
    "    return F1.tolist()  # Convert F1 scores from tensor to list\n",
    "\n",
    "# Define response styles and corresponding request/response data\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "requests_list = [req_c, req_f, req_e, req_p]   # Pre-defined lists of requests per style\n",
    "responses_list = [res_c, res_f, res_e, res_p]  # Corresponding list of responses per style\n",
    "\n",
    "# Initialize report containers\n",
    "bertscore_report = {style: {} for style in styles}  # For storing average F1 scores\n",
    "bert_scores = {}  # For storing raw F1 scores per instance\n",
    "\n",
    "# Compute BERTScore F1 for each style's request-response pairs\n",
    "for style, reqs, resps in zip(styles, requests_list, responses_list):\n",
    "    f1_scores = compute_bert_f1_scores(reqs, resps)\n",
    "    bertscore_report[style][\"bert_f1\"] = np.mean(f1_scores)  # Store average score\n",
    "    bert_scores[style] = f1_scores  # Store individual scores\n",
    "\n",
    "# Append results to output file\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nSemantic REQ-RES Overlap (BERTScore (F1), average)\", file=file)\n",
    "\n",
    "    # Perform Shapiro-Wilk test to check normality of score distributions\n",
    "    shapiro_p = {s: shapiro(bert_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())  # Are all distributions normal?\n",
    "\n",
    "    # Choose appropriate statistical test based on normality\n",
    "    data = [bert_scores[s] for s in styles]\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"\n",
    "        stat, p_val = f_oneway(*data)  # Use one-way ANOVA if all are normal\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"\n",
    "        stat, p_val = kruskal(*data)  # Otherwise, use non-parametric Kruskal-Wallis test\n",
    "\n",
    "    # Report type of test and its p-value\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # If significant differences exist between styles, conduct post-hoc analysis\n",
    "    if p_val < 0.05:\n",
    "        # Prepare data for pairwise post-hoc test\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s]*len(bert_scores[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Conduct Dunn's test with Bonferroni correction for multiple comparisons\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Summarize and annotate significant differences between styles\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Output average scores and significant differences\n",
    "        for s in styles:\n",
    "            mean_val = bertscore_report[s][\"bert_f1\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If no significant difference, simply print the average scores\n",
    "        for s in styles:\n",
    "            mean_val = bertscore_report[s][\"bert_f1\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === N-GRAM REQUEST/RESPONSE OVERLAP ANALYSIS (ROUGE-L) ===\n",
    "\n",
    "# Function to compute ROUGE-L F1 scores for each request/response pair\n",
    "def compute_rouge_l_scores(requests, responses):\n",
    "    # Initialize ROUGE-L scorer with stemming enabled\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    # Compute ROUGE-L F1 score for each (request, response) pair\n",
    "    return [\n",
    "        scorer.score(req, resp)['rougeL'].fmeasure\n",
    "        for req, resp in zip(requests, responses)\n",
    "    ]\n",
    "\n",
    "# List of different stylistic categories being evaluated\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# Corresponding request and response sets for each style\n",
    "requests_list = [req_c, req_f, req_e, req_p]   # Lists of requests by style\n",
    "responses_list = [res_c, res_f, res_e, res_p]  # Lists of responses by style\n",
    "\n",
    "# Initialize dictionaries to hold the results\n",
    "rouge_report = {style: {} for style in styles}  # Holds average ROUGE-L per style\n",
    "rouge_scores = {}  # Holds individual ROUGE-L scores per request/response pair per style\n",
    "\n",
    "# Compute ROUGE-L scores for each style\n",
    "for style, reqs, resps in zip(styles, requests_list, responses_list):\n",
    "    scores = compute_rouge_l_scores(reqs, resps)  # Compute per-pair scores\n",
    "    rouge_report[style][\"rouge_l\"] = np.mean(scores)  # Store average score\n",
    "    rouge_scores[style] = scores  # Store all individual scores\n",
    "\n",
    "# Write results to a file\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nN-gram REQ-RES Overlap (ROUGE-L, average)\", file=file)\n",
    "\n",
    "    # Test for normality using Shapiro-Wilk test\n",
    "    shapiro_p = {s: shapiro(rouge_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())  # Check if all styles are normally distributed\n",
    "\n",
    "    # Choose appropriate statistical test based on normality\n",
    "    data = [rouge_scores[s] for s in styles]\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"  # Parametric test for normally distributed data\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"  # Non-parametric test for non-normal data\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    # Report which statistical test was used and its p-value\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    if p_val < 0.05:\n",
    "        # If significant differences exist, perform post-hoc analysis\n",
    "        all_vals = [val for sublist in data for val in sublist]  # Flatten scores\n",
    "        all_labels = sum([[s]*len(rouge_scores[s]) for s in styles], [])  # Create labels per score\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})  # Create DataFrame for analysis\n",
    "\n",
    "        # Perform Dunn's post-hoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Generate significance annotations for each style\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Print mean scores along with significance markers\n",
    "        for s in styles:\n",
    "            mean_val = rouge_report[s][\"rouge_l\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If no significant difference, just print the means\n",
    "        for s in styles:\n",
    "            mean_val = rouge_report[s][\"rouge_l\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Deixis and Modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\n=== DEICTICS AND MODALITY ===\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROXIMAL DEICTIC RATIO ANALYSIS ===\n",
    "\n",
    "# Function to compute the ratio of proximal deictic terms per response\n",
    "def compute_deictic_ratios_per_text(responses: List[str], nlp) -> List[float]:\n",
    "    # Define sets of proximal and distal deictic terms\n",
    "    proximal = {\"this\", \"these\", \"here\"}\n",
    "\n",
    "    ratios = []\n",
    "    for text in responses:\n",
    "        # Process each response using the provided NLP pipeline\n",
    "        doc = nlp(text)\n",
    "        count_proximal = 0\n",
    "        count_distal = 0\n",
    "\n",
    "        # Iterate through tokens in the text\n",
    "        for token in doc:\n",
    "            if token.is_alpha:  # Only consider alphabetic tokens\n",
    "                tok = token.text.lower()\n",
    "\n",
    "                # Count proximal deictic terms\n",
    "                if tok in proximal:\n",
    "                    count_proximal += 1\n",
    "\n",
    "                # Count distal deictic terms with POS conditionals\n",
    "                elif tok == \"that\":\n",
    "                    if token.pos_ == \"DET\":  # \"that\" used as a determiner\n",
    "                        count_distal += 1\n",
    "                elif tok == \"there\":\n",
    "                    if token.pos_ != \"EX\":  # Exclude existential \"there\"\n",
    "                        count_distal += 1\n",
    "                elif tok == \"those\":\n",
    "                    count_distal += 1\n",
    "\n",
    "        # Calculate proximal deictic ratio (proximal / total)\n",
    "        total = count_proximal + count_distal\n",
    "        ratio = count_proximal / total if total > 0 else 0.0\n",
    "        ratios.append(ratio)\n",
    "\n",
    "    return ratios  # Return list of ratios, one per text\n",
    "\n",
    "# === PREPARE INPUTS AND OUTPUT CONTAINERS ===\n",
    "\n",
    "# Define the stylistic categories to analyze\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# Corresponding list of responses for each style\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Containers for computed ratios and summary results\n",
    "deictic_report = {style: {} for style in styles}  # Holds average ratio per style\n",
    "deictic_ratios = {}  # Holds individual ratios for each response\n",
    "\n",
    "# === COMPUTE RATIOS PER STYLE ===\n",
    "\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    # Compute proximal deictic ratios for each response\n",
    "    ratios = compute_deictic_ratios_per_text(resps, nlp=nlp)\n",
    "    # Store average ratio for current style\n",
    "    deictic_report[style][\"proximal_deictic_ratio\"] = np.mean(ratios)\n",
    "    # Store all ratios for further analysis\n",
    "    deictic_ratios[style] = ratios\n",
    "\n",
    "# === REPORT OUTPUT TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nProximal deictic ratio (corpus-level)\", file=file)\n",
    "\n",
    "    # Extract data for statistical testing\n",
    "    data = [deictic_ratios[s] for s in styles]\n",
    "\n",
    "    # Test for normality using Shapiro-Wilk test for each style\n",
    "    shapiro_p = {s: shapiro(deictic_ratios[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())  # Check if all are normally distributed\n",
    "\n",
    "    # Select appropriate test based on normality\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"  # Parametric test for normally distributed data\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"  # Non-parametric test for non-normal data\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    # Print which statistical test was used and the resulting p-value\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # If significant differences are found, perform post-hoc comparisons\n",
    "    if p_val < 0.05:\n",
    "        # Flatten the list of scores and create labels for each score\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s]*len(deictic_ratios[s]) for s in styles], [])\n",
    "        # Create DataFrame for post-hoc analysis\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "        # Run Dunn's test with Bonferroni correction for multiple comparisons\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "        # Annotate significance results\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Print average ratios with significance annotations\n",
    "        for s in styles:\n",
    "            mean_val = deictic_report[s][\"proximal_deictic_ratio\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If no significant difference, just print the average ratios\n",
    "        for s in styles:\n",
    "            mean_val = deictic_report[s][\"proximal_deictic_ratio\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FIRST/SECOND PERSON PRONOUN TYPE RATIOS ANALYSIS ===\n",
    "\n",
    "# Function to compute per-text ratios of first-person singular, first-person plural, and second-person pronouns\n",
    "def compute_pronoun_type_ratios(responses: List[str], nlp):\n",
    "    # Manual mapping of pronouns to pronoun types\n",
    "    manual_pronoun_map = {\n",
    "        \"i\": \"1|Sing\", \"me\": \"1|Sing\", \"my\": \"1|Sing\", \"mine\": \"1|Sing\", \"myself\": \"1|Sing\",\n",
    "        \"we\": \"1|Plur\", \"us\": \"1|Plur\", \"our\": \"1|Plur\", \"ours\": \"1|Plur\", \"ourselves\": \"1|Plur\",\n",
    "        \"you\": \"2|Sing\", \"your\": \"2|Sing\", \"yours\": \"2|Sing\", \"yourself\": \"2|Sing\", \"yourselves\": \"2|Plur\"\n",
    "    }\n",
    "\n",
    "    # Initialize lists to collect ratios for each type across all responses\n",
    "    ratios = {\"1|Sing\": [], \"1|Plur\": [], \"2\": []}\n",
    "\n",
    "    # Process each individual response\n",
    "    for text in responses:\n",
    "        doc = nlp(text)  # Run NLP pipeline on the text\n",
    "        counts = {\"1|Sing\": 0, \"1|Plur\": 0, \"2\": 0}  # Initialize counts per type\n",
    "\n",
    "        # Iterate through all tokens\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"PRON\":  # Only consider pronouns\n",
    "                key = manual_pronoun_map.get(token.text.lower())  # Map to type\n",
    "                if key:\n",
    "                    # Increment the appropriate count\n",
    "                    if key.startswith(\"1|Sing\"):\n",
    "                        counts[\"1|Sing\"] += 1\n",
    "                    elif key.startswith(\"1|Plur\"):\n",
    "                        counts[\"1|Plur\"] += 1\n",
    "                    elif key.startswith(\"2\"):\n",
    "                        counts[\"2\"] += 1\n",
    "\n",
    "        # Compute ratios over the sum of first- and second-person pronouns\n",
    "        total = sum(counts.values())\n",
    "        for k in counts:\n",
    "            ratios[k].append(counts[k] / total if total > 0 else 0.0)\n",
    "\n",
    "    return ratios  # Return per-text ratio lists for each pronoun type\n",
    "\n",
    "# === SETUP: STYLES AND CONTAINERS ===\n",
    "\n",
    "# Define the stylistic categories\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# Corresponding responses for each style\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Container to hold average pronoun ratios per style\n",
    "pronoun_type_report = {style: {} for style in styles}\n",
    "\n",
    "# Container to hold individual ratios per style, split by pronoun type\n",
    "pronoun_type_scores = {\"1|Sing\": {}, \"1|Plur\": {}, \"2\": {}}\n",
    "\n",
    "# === COMPUTE PRONOUN RATIOS PER STYLE ===\n",
    "\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    # Compute per-text ratios for the current style\n",
    "    style_ratios = compute_pronoun_type_ratios(resps, nlp=nlp)\n",
    "\n",
    "    # Store average and per-response ratios\n",
    "    for k in style_ratios:\n",
    "        # Save the mean ratio for reporting\n",
    "        pronoun_type_report[style][f\"pronoun_ratio_{k}\"] = np.mean(style_ratios[k])\n",
    "        # Save individual ratios for statistical analysis\n",
    "        pronoun_type_scores[k][style] = style_ratios[k]\n",
    "\n",
    "# === REPORT OUTPUT TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nPronoun type ratio (average)\", file=file)\n",
    "\n",
    "    # Loop through each pronoun type separately\n",
    "    for pron_type in [\"1|Sing\", \"1|Plur\", \"2\"]:\n",
    "        print(f\"\\n\\tPronoun type: {pron_type}\", file=file)\n",
    "\n",
    "        # Extract the per-text data for this pronoun type\n",
    "        data = [pronoun_type_scores[pron_type][s] for s in styles]\n",
    "\n",
    "        # Perform normality check using Shapiro-Wilk test\n",
    "        shapiro_p = {s: shapiro(pronoun_type_scores[pron_type][s]).pvalue for s in styles}\n",
    "        all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "\n",
    "        # Select test based on normality\n",
    "        if all_normal:\n",
    "            test_type = \"ANOVA\"  # Parametric test\n",
    "            stat, p_val = f_oneway(*data)\n",
    "        else:\n",
    "            test_type = \"Kruskal-Wallis\"  # Non-parametric test\n",
    "            stat, p_val = kruskal(*data)\n",
    "\n",
    "        # Output the test type and p-value\n",
    "        print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "        # If statistically significant, run post-hoc analysis\n",
    "        if p_val < 0.05:\n",
    "            # Flatten values and assign style labels\n",
    "            all_vals = [val for sublist in data for val in sublist]\n",
    "            all_labels = sum([[s]*len(pronoun_type_scores[pron_type][s]) for s in styles], [])\n",
    "            df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "            # Run Dunn's post-hoc test with Bonferroni correction\n",
    "            posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "            # Get annotation of significant differences\n",
    "            sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "            # Print average ratio with significance marker\n",
    "            for s in styles:\n",
    "                mean_val = pronoun_type_report[s][f\"pronoun_ratio_{pron_type}\"]\n",
    "                sig_diff = sig_labels[s]\n",
    "                print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "        else:\n",
    "            # No significant difference; print average ratio only\n",
    "            for s in styles:\n",
    "                mean_val = pronoun_type_report[s][f\"pronoun_ratio_{pron_type}\"]\n",
    "                print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRESENT TENSE RATIO ANALYSIS ===\n",
    "\n",
    "# Function to compute the ratio of present tense verbs per response\n",
    "def compute_present_tense_ratios(responses: List[str], nlp) -> List[float]:\n",
    "    # Set of POS tags corresponding to present tense forms\n",
    "    present_tags = {\"VBP\", \"VBZ\", \"VBG\", \"VB\"}  # e.g., \"run\", \"runs\", \"running\", \"to run\"\n",
    "    # Set of all verb-related POS tags (used to compute total verb count)\n",
    "    all_verb_tags = {\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}\n",
    "\n",
    "    ratios = []  # List to hold the present tense ratio for each response\n",
    "\n",
    "    for text in responses:\n",
    "        doc = nlp(text)  # Apply NLP pipeline to get POS tags\n",
    "        present_count = 0\n",
    "        verb_count = 0\n",
    "\n",
    "        # Iterate through each token and count verbs\n",
    "        for token in doc:\n",
    "            if token.tag_ in all_verb_tags:  # Is it a verb?\n",
    "                verb_count += 1\n",
    "                if token.tag_ in present_tags:  # Is it present tense?\n",
    "                    present_count += 1\n",
    "\n",
    "        # Compute the ratio of present tense verbs to all verbs\n",
    "        ratio = present_count / verb_count if verb_count > 0 else 0.0\n",
    "        ratios.append(ratio)\n",
    "\n",
    "    return ratios  # Return list of ratios (one per response)\n",
    "\n",
    "# === INITIALIZE STYLE DATA ===\n",
    "\n",
    "# Define different communication styles\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# Corresponding list of responses for each style\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Containers for storing results\n",
    "present_report = {style: {} for style in styles}  # Stores average present-tense ratio per style\n",
    "present_ratios = {}  # Stores per-text present-tense ratios per style\n",
    "\n",
    "# === COMPUTE RATIOS FOR EACH STYLE ===\n",
    "\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    # Compute per-response present tense ratios for the given style\n",
    "    ratios = compute_present_tense_ratios(resps, nlp=nlp)\n",
    "    present_report[style][\"present_tense_ratio\"] = np.mean(ratios)  # Store mean ratio\n",
    "    present_ratios[style] = ratios  # Store all individual ratios\n",
    "\n",
    "# === REPORT RESULTS TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nPresent tense ratio (average)\", file=file)\n",
    "\n",
    "    # Prepare data for statistical testing\n",
    "    data = [present_ratios[s] for s in styles]\n",
    "\n",
    "    # Test for normality using Shapiro-Wilk test\n",
    "    shapiro_p = {s: shapiro(present_ratios[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "\n",
    "    # Choose appropriate test based on normality result\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"  # Parametric test for normally distributed data\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"  # Non-parametric test for non-normal data\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    # Output the test type and p-value\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # If test shows significant differences, perform post-hoc comparisons\n",
    "    if p_val < 0.05:\n",
    "        # Flatten all scores and generate corresponding style labels\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s]*len(present_ratios[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Run Dunnâ€™s post-hoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Get annotated summary of significant differences\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Print average ratio with significance annotations\n",
    "        for s in styles:\n",
    "            mean_val = present_report[s][\"present_tense_ratio\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If no significant difference, print means only\n",
    "        for s in styles:\n",
    "            mean_val = present_report[s][\"present_tense_ratio\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONDITIONALS RATIO ANALYSIS ===\n",
    "\n",
    "# Function to compute how often conditional modal verbs appear in each text\n",
    "def compute_conditionals_per_text(responses: List[str], nlp) -> List[float]:\n",
    "    # Set of modal verbs commonly used to express conditionals\n",
    "    modal_verbs = {\"would\", \"could\", \"should\", \"might\", \"may\"}\n",
    "    # Include common negated forms of modal verbs\n",
    "    negated_modals = {\"wouldn't\", \"couldn't\", \"shouldn't\"}\n",
    "\n",
    "    counts = []  # List to store conditional counts per response\n",
    "\n",
    "    for text in responses:\n",
    "        doc = nlp(text)  # Run NLP pipeline to tokenize and tag\n",
    "        count = 0  # Initialize count of conditionals in this text\n",
    "\n",
    "        for token in doc:\n",
    "            text_lower = token.text.lower()\n",
    "\n",
    "            # Check for modal or negated modal verbs\n",
    "            if text_lower in modal_verbs or text_lower in negated_modals:\n",
    "                count += 1\n",
    "            # Special case: contracted form \"'d\" + verb (e.g., \"I'd go\")\n",
    "            elif text_lower == \"'d\":\n",
    "                next_token = token.nbor(1) if token.i + 1 < len(doc) else None\n",
    "                if next_token and next_token.tag_ == \"VB\":  # Ensure itâ€™s followed by a base verb\n",
    "                    count += 1\n",
    "\n",
    "        counts.append(count)  # Store count for this response\n",
    "\n",
    "    return counts  # Return list of counts per response\n",
    "\n",
    "# === SETUP: STYLES AND CONTAINERS ===\n",
    "\n",
    "# Define communication styles to analyze\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# Corresponding responses for each style\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Initialize containers for average conditional usage and per-response counts\n",
    "conditional_report = {style: {} for style in styles}\n",
    "conditional_ratios = {}\n",
    "\n",
    "# === COMPUTE CONDITIONAL USAGE PER STYLE ===\n",
    "\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    counts = compute_conditionals_per_text(resps, nlp=nlp)  # Count conditionals per response\n",
    "    conditional_report[style][\"conditionals_per_text\"] = np.mean(counts)  # Store average per text\n",
    "    conditional_ratios[style] = counts  # Store all counts for later statistical analysis\n",
    "\n",
    "# === REPORT OUTPUT TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nConditionals per text (average)\", file=file)\n",
    "\n",
    "    # Extract data for statistical analysis\n",
    "    data = [conditional_ratios[s] for s in styles]\n",
    "\n",
    "    # Check for normality of each group's data using Shapiro-Wilk test\n",
    "    shapiro_p = {s: shapiro(conditional_ratios[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())  # Determine if parametric test is appropriate\n",
    "\n",
    "    # Choose statistical test based on normality result\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"  # Use ANOVA if all groups are normally distributed\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"  # Use Kruskal-Wallis if not all groups are normal\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    # Report test type and p-value\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # If significant differences found, perform post-hoc analysis\n",
    "    if p_val < 0.05:\n",
    "        # Flatten all values and assign style labels to each\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s] * len(conditional_ratios[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Run Dunn's post-hoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Annotate significant differences\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Print average count and significance annotation for each style\n",
    "        for s in styles:\n",
    "            mean_val = conditional_report[s][\"conditionals_per_text\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If no significant differences, print only the means\n",
    "        for s in styles:\n",
    "            mean_val = conditional_report[s][\"conditionals_per_text\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment and Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\n=== SENTIMENT AND EMOTION ===\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SENTIMENT ANALYSIS ===\n",
    "\n",
    "# Function to compute sentiment scores for a list of texts using batch processing\n",
    "def compute_sentiment_scores(texts: List[str], batch_size: int = 32) -> List[float]:\n",
    "    # Load pipeline once outside loop\n",
    "    pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", top_k=None, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "    # Define label to score mapping\n",
    "    label_scores = {'LABEL_0': 0, 'LABEL_1': 0.5, 'LABEL_2': 1}\n",
    "\n",
    "    # Process in batches for efficiency\n",
    "    results = pipe(texts, batch_size=batch_size)\n",
    "\n",
    "    # Compute weighted average score for each item in batch\n",
    "    sentiment_values = [\n",
    "        sum(label_scores[entry['label']] * entry['score'] for entry in result)\n",
    "        for result in results\n",
    "    ]\n",
    "\n",
    "    return sentiment_values\n",
    "\n",
    "# === SETUP: CONTAINERS AND STYLES ===\n",
    "\n",
    "# List of styles to analyze\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# Corresponding request and response sets\n",
    "requests_list = [req_c, req_f, req_e, req_p]\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Dictionary to hold mean sentiment scores per style\n",
    "sentiment_report = {style: {} for style in styles}\n",
    "\n",
    "# Store individual sentiment scores for statistical testing\n",
    "sentiment_scores_requests = {}\n",
    "sentiment_scores_responses = {}\n",
    "\n",
    "# === COMPUTE SENTIMENT SCORES FOR REQUESTS AND RESPONSES ===\n",
    "\n",
    "for style, reqs, resps in zip(styles, requests_list, responses_list):\n",
    "    req_scores = compute_sentiment_scores(reqs)  # Sentiment for requests\n",
    "    res_scores = compute_sentiment_scores(resps)  # Sentiment for responses\n",
    "\n",
    "    # Store mean sentiment per style\n",
    "    sentiment_report[style][\"sentiment_requests\"] = np.mean(req_scores)\n",
    "    sentiment_report[style][\"sentiment_responses\"] = np.mean(res_scores)\n",
    "\n",
    "    # Store individual scores for statistical testing\n",
    "    sentiment_scores_requests[style] = req_scores\n",
    "    sentiment_scores_responses[style] = res_scores\n",
    "\n",
    "# === FUNCTION: BETWEEN-STYLES COMPARISON ===\n",
    "\n",
    "# Run analysis across styles and output to file\n",
    "def report_sentiment_analysis(scores_dict, label, file, report_key):\n",
    "    data = [scores_dict[s] for s in styles]  # Gather scores by style\n",
    "\n",
    "    # Test for normality using Shapiro-Wilk test\n",
    "    shapiro_p = {s: shapiro(scores_dict[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "\n",
    "    # Choose statistical test based on normality\n",
    "    test_type = \"ANOVA\" if all_normal else \"Kruskal-Wallis\"\n",
    "    stat, p_val = (f_oneway(*data) if all_normal else kruskal(*data))\n",
    "\n",
    "    # Print test summary\n",
    "    print(f\"\\nSentiment Score ({label.upper()}, average)\", file=file)\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # If results are statistically significant, perform post-hoc test\n",
    "    if p_val < 0.05:\n",
    "        # Flatten data and create corresponding labels\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s] * len(scores_dict[s]) for s in styles], [])\n",
    "\n",
    "        # Create DataFrame for analysis\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Run Dunnâ€™s post-hoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Summarize significant pairwise differences\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Output mean sentiment and significance annotations\n",
    "        for s in styles:\n",
    "            mean_val = sentiment_report[s][report_key]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If not significant, just report the means\n",
    "        for s in styles:\n",
    "            mean_val = sentiment_report[s][report_key]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)\n",
    "\n",
    "# === FUNCTION: WITHIN-STYLE COMPARISON (REQUEST VS RESPONSE) ===\n",
    "\n",
    "# Compare request vs. response sentiment within each style\n",
    "def report_within_style_comparisons(file):\n",
    "    print(\"\\nSentiment Score: Request vs. Response (within style)\\n\", file=file)\n",
    "\n",
    "    for s in styles:\n",
    "        req = sentiment_scores_requests[s]\n",
    "        res = sentiment_scores_responses[s]\n",
    "\n",
    "        # Test normality for both groups\n",
    "        p_req = shapiro(req).pvalue\n",
    "        p_res = shapiro(res).pvalue\n",
    "\n",
    "        # Choose appropriate test\n",
    "        if p_req >= 0.05 and p_res >= 0.05:\n",
    "            test = \"t-test\"\n",
    "            stat, pval = ttest_ind(req, res)\n",
    "        else:\n",
    "            test = \"Rank-sum\"\n",
    "            stat, pval = ranksums(req, res)\n",
    "\n",
    "        # Output result of within-style test\n",
    "        print(f\"\\t{s:<12}: {test:<8} (p = {pval:.4f})\", file=file)\n",
    "\n",
    "# === OUTPUT RESULTS TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    # Compare sentiment across styles for requests and responses separately\n",
    "    report_sentiment_analysis(sentiment_scores_requests, \"Request\", file, \"sentiment_requests\")\n",
    "    report_sentiment_analysis(sentiment_scores_responses, \"Response\", file, \"sentiment_responses\")\n",
    "\n",
    "    # Compare request vs. response sentiment within each style\n",
    "    report_within_style_comparisons(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EMOTION ANALYSIS ===\n",
    "\n",
    "def compute_emotion_scores(responses: List[str], batch_size: int = 32) -> Dict[str, List[float]]:\n",
    "    # Load pipeline once with GPU if available\n",
    "    pipe = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "        top_k=None,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "\n",
    "    # Run batch inference\n",
    "    results = pipe(responses, batch_size=batch_size)\n",
    "\n",
    "    # Store scores for each emotion label across all responses\n",
    "    emotion_lists = defaultdict(list)\n",
    "\n",
    "    for response_result in results:\n",
    "        for item in response_result:\n",
    "            emotion_lists[item['label']].append(item['score'])\n",
    "\n",
    "    return emotion_lists  # Dict[emotion â†’ list of scores]\n",
    "\n",
    "# === INITIALIZATION ===\n",
    "\n",
    "# Define styles\n",
    "styles = [\"clear\", \"friendly\", \"empath.\", \"polite\"]\n",
    "\n",
    "# Response and request lists by style\n",
    "requests_list = [req_c, req_f, req_e, req_p]\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Containers to store average scores\n",
    "emotion_report_resp = {style: {} for style in styles}  # Emotion means for responses\n",
    "emotion_report_req = {style: {} for style in styles}  # Emotion means for requests\n",
    "\n",
    "# Containers to store raw scores\n",
    "emotion_scores_resp = defaultdict(dict)  # Dict[emotion][style] â†’ list of response scores\n",
    "emotion_scores_req = defaultdict(dict)  # Dict[emotion][style] â†’ list of request scores\n",
    "\n",
    "# === COMPUTE EMOTION SCORES FOR EACH STYLE ===\n",
    "\n",
    "for style, reqs, resps in zip(styles, requests_list, responses_list):\n",
    "    # Get emotion scores (per response/request) for each style\n",
    "    emotion_lists_resp = compute_emotion_scores(resps)\n",
    "    emotion_lists_req = compute_emotion_scores(reqs)\n",
    "\n",
    "    # Store average and raw scores for responses\n",
    "    for emotion, scores in emotion_lists_resp.items():\n",
    "        emotion_report_resp[style][emotion] = np.mean(scores)\n",
    "        emotion_scores_resp[emotion][style] = scores\n",
    "\n",
    "    # Store average and raw scores for requests\n",
    "    for emotion, scores in emotion_lists_req.items():\n",
    "        emotion_report_req[style][emotion] = np.mean(scores)\n",
    "        emotion_scores_req[emotion][style] = scores\n",
    "\n",
    "# === REPORT EMOTION SCORES TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    # --- RESPONSE-BASED EMOTIONS ---\n",
    "\n",
    "    print(\"\\nEmotion Score (responses, average)\\n\", file=file)\n",
    "    header = f\"{'Emotion':<14}\" + \"\".join([f\"{s:<10}\" for s in styles]) + \"  HM   \"\n",
    "    print(header, file=file)\n",
    "    print(\"-\" * len(header), file=file)\n",
    "\n",
    "    for emotion in sorted(emotion_scores_resp.keys()):\n",
    "        # Skip if emotion not present for all styles\n",
    "        data = [emotion_scores_resp[emotion][s] for s in styles if s in emotion_scores_resp[emotion]]\n",
    "        if len(data) < len(styles):\n",
    "            continue\n",
    "\n",
    "        # Statistical testing (ANOVA or Kruskal-Wallis depending on normality)\n",
    "        shapiro_p = {s: shapiro(emotion_scores_resp[emotion][s]).pvalue for s in styles}\n",
    "        all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "        test_type = \"ANOVA\" if all_normal else \"Kruskal-Wallis\"\n",
    "        stat, p_val = (f_oneway(*data) if all_normal else kruskal(*data))\n",
    "\n",
    "        # Get means per style\n",
    "        means = [emotion_report_resp[s].get(emotion, 0.0) for s in styles]\n",
    "\n",
    "        # Post-hoc if significant\n",
    "        if p_val < 0.05:\n",
    "            all_vals = [val for sublist in data for val in sublist]\n",
    "            all_labels = sum([[s] * len(emotion_scores_resp[emotion][s]) for s in styles], [])\n",
    "            df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "            posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "            sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "            annotations = [sig_labels[s] for s in styles]\n",
    "        else:\n",
    "            annotations = [\"\" for _ in styles]\n",
    "\n",
    "        # Harmonic mean (as diversity/consistency metric)\n",
    "        hm = len(means) / sum(1/m for m in means if m > 0) if all(m > 0 for m in means) else 0\n",
    "\n",
    "        # Format row for output\n",
    "        row = f\"{emotion:<14}\" + \"\".join([f\"{m:.3f} {a:<4}\" for m, a in zip(means, annotations)]) + f\"{hm:>7.3f}\"\n",
    "        print(row, file=file)\n",
    "\n",
    "    # --- REQUEST-BASED EMOTIONS ---\n",
    "\n",
    "    print(\"\\nEmotion Score (requests, average)\\n\", file=file)\n",
    "    print(header, file=file)\n",
    "    print(\"-\" * len(header), file=file)\n",
    "\n",
    "    for emotion in sorted(emotion_scores_req.keys()):\n",
    "        data = [emotion_scores_req[emotion][s] for s in styles if s in emotion_scores_req[emotion]]\n",
    "        if len(data) < len(styles):\n",
    "            continue\n",
    "\n",
    "        shapiro_p = {s: shapiro(emotion_scores_req[emotion][s]).pvalue for s in styles}\n",
    "        all_normal = all(p >= 0.05 for p in shapiro_p.values())\n",
    "        test_type = \"ANOVA\" if all_normal else \"Kruskal-Wallis\"\n",
    "        stat, p_val = (f_oneway(*data) if all_normal else kruskal(*data))\n",
    "\n",
    "        means = [emotion_report_req[s].get(emotion, 0.0) for s in styles]\n",
    "\n",
    "        if p_val < 0.05:\n",
    "            all_vals = [val for sublist in data for val in sublist]\n",
    "            all_labels = sum([[s] * len(emotion_scores_req[emotion][s]) for s in styles], [])\n",
    "            df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "            posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "            sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "            annotations = [sig_labels[s] for s in styles]\n",
    "        else:\n",
    "            annotations = [\"\" for _ in styles]\n",
    "\n",
    "        hm = len(means) / sum(1/m for m in means if m > 0) if all(m > 0 for m in means) else 0\n",
    "        row = f\"{emotion:<14}\" + \"\".join([f\"{m:.3f} {a:<4}\" for m, a in zip(means, annotations)]) + f\"{hm:>7.3f}\"\n",
    "        print(row, file=file)\n",
    "\n",
    "    # --- WITHIN-STYLE COMPARISON (REQUEST vs RESPONSE) ---\n",
    "\n",
    "    print(\"\\nEmotion Score (within style)\\n\", file=file)\n",
    "    header_within = f\"{'Emotion':<14}\" + \"\".join([f\"{s:<10}\" for s in styles])\n",
    "    print(header_within, file=file)\n",
    "    print(\"-\" * len(header_within), file=file)\n",
    "\n",
    "    for emotion in sorted(emotion_scores_resp.keys()):\n",
    "        row = f\"{emotion:<14}\"\n",
    "        for s in styles:\n",
    "            req = emotion_scores_req[emotion].get(s, None)\n",
    "            res = emotion_scores_resp[emotion].get(s, None)\n",
    "            if not req or not res:\n",
    "                row += f\"{'N/A':<14}\"\n",
    "                continue\n",
    "\n",
    "            # Choose test based on normality of each group\n",
    "            p_req = shapiro(req).pvalue\n",
    "            p_res = shapiro(res).pvalue\n",
    "            if p_req >= 0.05 and p_res >= 0.05:\n",
    "                test_abbr = \"TT\"  # t-test\n",
    "                _, pval = ttest_ind(req, res)\n",
    "            else:\n",
    "                test_abbr = \"RS\"  # Rank-sum (non-parametric)\n",
    "                _, pval = ranksums(req, res)\n",
    "\n",
    "            star = \"*\" if pval < 0.05 else \"\"  # Mark significance\n",
    "            row += f\"{test_abbr}{star:<8}\"  # Add test abbreviation and sig marker\n",
    "        print(row, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\n=== ADDITIONAL SCORES ===\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FLESCH-KINCAID READABILITY ANALYSIS ===\n",
    "\n",
    "# Function to compute Flesch Reading Ease scores for a list of texts\n",
    "def compute_fk_scores(responses: List[str]) -> List[float]:\n",
    "    # Applies the Flesch Reading Ease formula to each response\n",
    "    return [textstat.flesch_reading_ease(text) for text in responses]\n",
    "\n",
    "# === SETUP: CONTAINERS AND STYLE GROUPS ===\n",
    "\n",
    "# List of communication styles to evaluate\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# Corresponding list of response sets by style\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Dictionary to store mean readability scores per style\n",
    "fk_report = {style: {} for style in styles}\n",
    "\n",
    "# Dictionary to store per-text FK scores for each style\n",
    "fk_scores = {}\n",
    "\n",
    "# === COMPUTE FK SCORES FOR EACH STYLE ===\n",
    "\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    scores = compute_fk_scores(resps)  # Compute FK score for each response\n",
    "    fk_report[style][\"flesch_kincaid\"] = np.mean(scores)  # Store average FK score\n",
    "    fk_scores[style] = scores  # Store individual scores for statistical analysis\n",
    "\n",
    "# === REPORT OUTPUT TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nReadibility Score (Flesch-Kincaid Reading Ease, average)\", file=file)\n",
    "\n",
    "    # Gather per-style score lists for statistical analysis\n",
    "    data = [fk_scores[s] for s in styles]\n",
    "\n",
    "    # Test each group's score distribution for normality (Shapiro-Wilk)\n",
    "    shapiro_p = {s: shapiro(fk_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())  # Check if all are normal\n",
    "\n",
    "    # Choose statistical test based on normality result\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"  # Parametric test for normal data\n",
    "        stat, p_val = f_oneway(*data)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"  # Non-parametric test for non-normal data\n",
    "        stat, p_val = kruskal(*data)\n",
    "\n",
    "    # Print test result\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # If significant differences are found, run post-hoc analysis\n",
    "    if p_val < 0.05:\n",
    "        # Flatten all scores and create corresponding style labels\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s] * len(fk_scores[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Run Dunn's post-hoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Summarize which pairs are significantly different\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Output average score with significance indicators\n",
    "        for s in styles:\n",
    "            mean_val = fk_report[s][\"flesch_kincaid\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # If no significant differences, just print average scores\n",
    "        for s in styles:\n",
    "            mean_val = fk_report[s][\"flesch_kincaid\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FORMALITY ANALYSIS ===\n",
    "\n",
    "# Function to compute binary formality scores for a list of texts\n",
    "def compute_formality_scores(responses: List[str]) -> List[float]:\n",
    "    # Load a pre-trained RoBERTa-based classifier for formality ranking\n",
    "    pipe = pipeline(\"text-classification\", model=\"s-nlp/roberta-base-formality-ranker\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "    scores = []  # List to store formality scores\n",
    "\n",
    "    for text in responses:\n",
    "        result = pipe(text)[0]['label']  # Get the predicted label for the text\n",
    "\n",
    "        # Assign score: 1.0 for \"formal\", 0.0 for \"informal\"\n",
    "        if result == \"formal\":\n",
    "            scores.append(1.0)\n",
    "        elif result == \"informal\":\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return scores  # Return list of binary formality scores\n",
    "\n",
    "# === INITIAL SETUP ===\n",
    "\n",
    "# Define communication styles being analyzed\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# Corresponding response sets for each style\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Containers to store mean formality scores and per-text scores\n",
    "formality_report = {style: {} for style in styles}  # Average score per style\n",
    "formality_scores = {}  # List of individual scores per style\n",
    "\n",
    "# === COMPUTE FORMALITY SCORES PER STYLE ===\n",
    "\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    scores = compute_formality_scores(resps)  # Get binary scores per response\n",
    "    formality_report[style][\"formality\"] = np.mean(scores)  # Store average formality score\n",
    "    formality_scores[style] = scores  # Store raw scores for statistical analysis\n",
    "\n",
    "# === OUTPUT RESULTS TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nFormality Score (average)\", file=file)\n",
    "\n",
    "    # Prepare data for statistical analysis\n",
    "    data = [formality_scores[s] for s in styles]\n",
    "\n",
    "    # Test for normality using Shapiro-Wilk\n",
    "    shapiro_p = {s: shapiro(formality_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())  # Check if all distributions are normal\n",
    "\n",
    "    # Choose statistical test based on normality\n",
    "    test_type = \"ANOVA\" if all_normal else \"Kruskal-Wallis\"\n",
    "    stat, p_val = (f_oneway(*data) if all_normal else kruskal(*data))\n",
    "\n",
    "    # Print overall test result\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # If statistically significant, perform post-hoc test\n",
    "    if p_val < 0.05:\n",
    "        # Flatten score data and assign labels\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s]*len(formality_scores[s]) for s in styles], [])\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Perform Dunn's post-hoc test with Bonferroni correction\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Summarize pairwise significance annotations\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Print average formality score with significance indicators\n",
    "        for s in styles:\n",
    "            mean_val = formality_report[s][\"formality\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "\n",
    "    else:\n",
    "        # If no significant differences, print average scores only\n",
    "        for s in styles:\n",
    "            mean_val = formality_report[s][\"formality\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONCRETENESS ANALYSIS ===\n",
    "\n",
    "# Function to compute concreteness scores for a list of responses\n",
    "def compute_concreteness_scores(responses: List[str], file: str, nlp) -> List[float]:\n",
    "    # --- Load and parse the concreteness lexicon from raw text ---\n",
    "    concreteness_dict = {}  # Dictionary to map words to their concreteness scores\n",
    "\n",
    "    for line in file.splitlines():\n",
    "        # Match lines with a word followed by a floating-point score\n",
    "        match = re.match(r\"^(\\w+)\\s+(\\d{1,3}(?:\\.\\d+)?)\", line.strip())\n",
    "        if match:\n",
    "            word = match.group(1).lower()\n",
    "            score = float(match.group(2))\n",
    "            concreteness_dict[word] = score  # Add to dictionary\n",
    "\n",
    "    scores = []  # List to store the concreteness score per response\n",
    "\n",
    "    for text in responses:\n",
    "        doc = nlp(text)  # Apply NLP processing (e.g., lemmatization)\n",
    "\n",
    "        # Get concreteness scores for each lemmatized token, if available\n",
    "        token_scores = [\n",
    "            concreteness_dict[token.lemma_.lower()]\n",
    "            for token in doc\n",
    "            if not token.is_punct and token.lemma_.lower() in concreteness_dict\n",
    "        ]\n",
    "\n",
    "        # Count number of non-punctuation tokens for normalization\n",
    "        num_tokens = len([t for t in doc if not t.is_punct])\n",
    "\n",
    "        # Compute average concreteness score, scaled to 0â€“1 range\n",
    "        score = sum(token_scores) / (num_tokens * 100) if num_tokens > 0 else 0\n",
    "        scores.append(score / 7)  # Divide by max possible score (7.0) to normalize\n",
    "\n",
    "    return scores  # Return concreteness scores per response\n",
    "\n",
    "# === INITIAL SETUP ===\n",
    "\n",
    "# Define the communication styles being analyzed\n",
    "styles = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# Corresponding responses for each style\n",
    "responses_list = [res_c, res_f, res_e, res_p]\n",
    "\n",
    "# Containers for results\n",
    "concreteness_report = {style: {} for style in styles}  # Average score per style\n",
    "concreteness_scores = {}  # Individual scores per style\n",
    "\n",
    "# === COMPUTE CONCRETENESS SCORES PER STYLE ===\n",
    "\n",
    "for style, resps in zip(styles, responses_list):\n",
    "    # Compute concreteness scores for all responses of a given style\n",
    "    scores = compute_concreteness_scores(resps, concreteness_txt, nlp)\n",
    "    concreteness_report[style][\"concreteness\"] = np.mean(scores)  # Store average\n",
    "    concreteness_scores[style] = scores  # Store raw scores\n",
    "\n",
    "# === OUTPUT RESULTS TO FILE ===\n",
    "\n",
    "with open('results.txt', 'a') as file:\n",
    "    print(\"\\nConcreteness Score (average)\", file=file)\n",
    "\n",
    "    # Prepare data for statistical testing\n",
    "    data = [concreteness_scores[s] for s in styles]\n",
    "\n",
    "    # Perform Shapiro-Wilk normality test for each style\n",
    "    shapiro_p = {s: shapiro(concreteness_scores[s]).pvalue for s in styles}\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p.values())  # Determine if all distributions are normal\n",
    "\n",
    "    # Select statistical test based on normality\n",
    "    test_type = \"ANOVA\" if all_normal else \"Kruskal-Wallis\"\n",
    "    stat, p_val = (f_oneway(*data) if all_normal else kruskal(*data))\n",
    "\n",
    "    # Print test summary\n",
    "    print(f\"\\n\\tStatistical Test: {test_type} (p = {p_val:.4f})\\n\", file=file)\n",
    "\n",
    "    # If significant differences are found, perform post-hoc analysis\n",
    "    if p_val < 0.05:\n",
    "        # Flatten the scores and assign style labels to each entry\n",
    "        all_vals = [val for sublist in data for val in sublist]\n",
    "        all_labels = sum([[s]*len(concreteness_scores[s]) for s in styles], [])\n",
    "\n",
    "        # Create a DataFrame for post-hoc analysis\n",
    "        df = pd.DataFrame({\"score\": all_vals, \"style\": all_labels})\n",
    "\n",
    "        # Run Dunnâ€™s test with Bonferroni correction for multiple comparisons\n",
    "        posthoc = sp.posthoc_dunn(df, val_col=\"score\", group_col=\"style\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        # Annotate significant differences\n",
    "        sig_labels = summarize_significant_differences_annotated(posthoc, styles)\n",
    "\n",
    "        # Print average score and significance annotations\n",
    "        for s in styles:\n",
    "            mean_val = concreteness_report[s][\"concreteness\"]\n",
    "            sig_diff = sig_labels[s]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}  {sig_diff}\", file=file)\n",
    "    else:\n",
    "        # No significant differences: print average scores only\n",
    "        for s in styles:\n",
    "            mean_val = concreteness_report[s][\"concreteness\"]\n",
    "            print(f\"\\t{s:<12}: {mean_val:.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Tasks\n",
    "\n",
    "In this section, different models are adapted on the RESPONSible Service dataset and tested for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Task with ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"HF_TOKEN\"] = \"HF_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Define the model ID for the text generation pipeline\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "def classify_response_style(test_set, icl_set, stylistic_feature, model_name, num_shots=0):\n",
    "    \"\"\"\n",
    "    Generates rephrased customer service responses using ICL (in-context learning)\n",
    "    to match a given stylistic feature (e.g., 'friendly', 'clear').\n",
    "\n",
    "    Parameters:\n",
    "    - test_set: DataFrame with examples to rephrase (must include 'response0')\n",
    "    - icl_set: DataFrame with few-shot examples including 'response0' and 'response1'\n",
    "    - stylistic_feature: the target style to enhance (e.g., \"empathetic\")\n",
    "    - model_name: name or path of the Hugging Face model to use\n",
    "    - num_shots: number of ICL examples to prepend to each prompt\n",
    "\n",
    "    Returns:\n",
    "    - A list of dictionaries containing 'id', 'response0', and 'generated_response1'\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a text generation pipeline using the specified model\n",
    "    generator = pipeline(\n",
    "        model=model_name,\n",
    "        max_new_tokens=40,  # Limit the output length\n",
    "        device = 0 if torch.cuda.is_available() else -1,  # Use GPU (device=0) if available\n",
    "        temperature=1.0,  # Sampling temperature\n",
    "        top_p=1.0  # Top-p sampling (nucleus sampling)\n",
    "    )\n",
    "\n",
    "    # === FEW-SHOT PROMPT CONSTRUCTION ===\n",
    "    few_shot_prompt = \"\"\n",
    "    for _, shot in icl_set.iloc[:num_shots].iterrows():\n",
    "        # Clean and extract the original and target responses\n",
    "        shot_response0 = str(shot[\"response0\"]).strip()\n",
    "        shot_response1 = str(shot[\"response1\"]).strip()\n",
    "\n",
    "        # Format the few-shot example to show how a response is made more stylistic\n",
    "        few_shot_prompt += (\n",
    "            f\"Instruction: A customer service agent responded to a message from a customer in the following way.\\n\"\n",
    "            f\"Response: {shot_response0}\\n\"\n",
    "            f\"Rephrase the response to make it more {stylistic_feature}. Only output the rephrased response. Do not add any explanation or extra text.\\n\"\n",
    "            f\"More {stylistic_feature} response: {shot_response1}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    # === CONSTRUCT PROMPTS FOR TEST SET ===\n",
    "    prompts = []\n",
    "    for _, row in test_set.iterrows():\n",
    "        response0 = str(row[\"response0\"]).strip()\n",
    "\n",
    "        # Append the few-shot prompt and the test response to create a full prompt\n",
    "        prompt = (\n",
    "            few_shot_prompt +\n",
    "            f\"Instruction: A customer service agent responded to a message from a customer in the following way.\\n\"\n",
    "            f\"Response: {response0}\\n\"\n",
    "            f\"Rephrase the response to make it more {stylistic_feature}. Only output the rephrased response. Do not add any explanation or extra text.\\n\"\n",
    "            f\"More {stylistic_feature} response:\"\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # === TEXT GENERATION ===\n",
    "    outputs = generator(prompts, batch_size=8)\n",
    "\n",
    "    # === EXTRACT GENERATED RESPONSES ===\n",
    "    predictions = []\n",
    "    for i, (row, out) in enumerate(zip(test_set.itertuples(), outputs)):\n",
    "        if isinstance(out, list) and isinstance(out[0], dict):\n",
    "            generated_text = out[0][\"generated_text\"]\n",
    "        elif isinstance(out, dict):\n",
    "            generated_text = out[\"generated_text\"]\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected output format from the generator\")\n",
    "\n",
    "        generated_response1 = generated_text.split(f\"More {stylistic_feature} response:\")[-1].strip()\n",
    "        if \"\\n\\nInstruction:\" in generated_response1:\n",
    "            generated_response1 = generated_response1.split(f\"\\n\\nInstruction:\")[0].strip()\n",
    "\n",
    "        prediction = {\n",
    "            \"id\": getattr(row, \"id\", i),\n",
    "            \"response0\": row.response0.strip(),\n",
    "            \"generated_response1\": generated_response1\n",
    "        }\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RETRIEVE PARQUET FILES ===\n",
    "# Each path points to a different stylistic dataset\n",
    "path_clear = \"RESPONSible Service/dataset_clear.parquet\"\n",
    "path_friendly = \"RESPONSible Service/dataset_friendly.parquet\"\n",
    "path_empathetic = \"RESPONSible Service/dataset_empathetic.parquet\"\n",
    "path_polite = \"RESPONSible Service/dataset_polite.parquet\"\n",
    "\n",
    "# Read the Parquet content into Pandas DataFrames\n",
    "clear_df = pd.read_parquet(path_clear)\n",
    "friendly_df = pd.read_parquet(path_friendly)\n",
    "empathetic_df = pd.read_parquet(path_empathetic)\n",
    "polite_df = pd.read_parquet(path_polite)\n",
    "\n",
    "# === SHUFFLE DATASETS (for randomness and reproducibility) ===\n",
    "# Shuffle each dataset using the same random seed and reset the index\n",
    "clear_df = clear_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "friendly_df = friendly_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "empathetic_df = empathetic_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "polite_df = polite_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === SELECT FEW-SHOT (ICL) EXAMPLES FROM FIRST 800 INSTANCES ===\n",
    "# Take 5 random examples (for ICL prompting) from the first 800 rows of each DataFrame\n",
    "clear_icl = clear_df.iloc[:800].sample(n=5, random_state=42).reset_index(drop=True)\n",
    "friendly_icl = friendly_df.iloc[:800].sample(n=5, random_state=42).reset_index(drop=True)\n",
    "empathetic_icl = empathetic_df.iloc[:800].sample(n=5, random_state=42).reset_index(drop=True)\n",
    "polite_icl = polite_df.iloc[:800].sample(n=5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === USE REMAINING DATA AS TEST SET ===\n",
    "# Use the rest (from row 800 onward) as test data for each stylistic category\n",
    "clear_test = clear_df.iloc[800:].reset_index(drop=True)\n",
    "friendly_test = friendly_df.iloc[800:].reset_index(drop=True)\n",
    "empathetic_test = empathetic_df.iloc[800:].reset_index(drop=True)\n",
    "polite_test = polite_df.iloc[800:].reset_index(drop=True)\n",
    "\n",
    "# === SETUP LISTS FOR LOOPING ===\n",
    "stylistic_features = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "test_sets = [clear_test, friendly_test, empathetic_test, polite_test]\n",
    "icl_sets = [clear_icl, friendly_icl, empathetic_icl, polite_icl]\n",
    "\n",
    "# === MAIN LOOP: RUN INFERENCE FOR EACH STYLE AND EACH FEW-SHOT SETTING ===\n",
    "all_results = {}\n",
    "\n",
    "for feature, test_set, icl_set in zip(stylistic_features, test_sets, icl_sets):\n",
    "    print(f\"Processing started: {feature}\\n\")\n",
    "    all_results[feature] = {}\n",
    "\n",
    "    # Test from 0-shot to 5-shot\n",
    "    for n_shot in range(6):\n",
    "        print(f\"\\tFew-shots stage: {n_shot}-shots\")\n",
    "        # Run classification/generation for current feature and few-shot setting\n",
    "        results = classify_response_style(test_set, icl_set, feature, model_id, num_shots=n_shot)\n",
    "        all_results[feature][f\"{n_shot}-shot\"] = results\n",
    "\n",
    "    print(f\"\\n\\tProcessing ended: {feature}\\n\")\n",
    "\n",
    "# === SAVE RESULTS TO JSON FILE ===\n",
    "# Write the nested dictionary containing all results to a local file\n",
    "with open(\"icl_predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Task with SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"HF_TOKEN\"] = \"HF_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from multiprocessing import cpu_count\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# === RETRIEVE PARQUET FILES ===\n",
    "# Each path points to a different stylistic dataset\n",
    "path_clear = \"RESPONSible Service/dataset_clear.parquet\"\n",
    "path_friendly = \"RESPONSible Service/dataset_friendly.parquet\"\n",
    "path_empathetic = \"RESPONSible Service/dataset_empathetic.parquet\"\n",
    "path_polite = \"RESPONSible Service/dataset_polite.parquet\"\n",
    "\n",
    "# Read the Parquet content into Pandas DataFrames\n",
    "clear_df = pd.read_parquet(path_clear)\n",
    "friendly_df = pd.read_parquet(path_friendly)\n",
    "empathetic_df = pd.read_parquet(path_empathetic)\n",
    "polite_df = pd.read_parquet(path_polite)\n",
    "\n",
    "# === SHUFFLE EACH DATAFRAME TO AVOID ORDER BIAS ===\n",
    "clear_df = clear_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "friendly_df = friendly_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "empathetic_df = empathetic_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "polite_df = polite_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === GROUP DATAFRAMES AND STYLE NAMES FOR ITERATION ===\n",
    "dfs = [clear_df, friendly_df, empathetic_df, polite_df]\n",
    "names = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]\n",
    "\n",
    "# === PREPARE A DICTIONARY TO HOLD TRAIN/TEST SPLITS IN CONVERSATION FORMAT ===\n",
    "raw_datasets_dict = {\n",
    "    \"clear\": {\"train\": None, \"test\": None},\n",
    "    \"friendly\": {\"train\": None, \"test\": None},\n",
    "    \"empathetic\": {\"train\": None, \"test\": None},\n",
    "    \"polite\": {\"train\": None, \"test\": None}\n",
    "}\n",
    "\n",
    "# === FORMAT EACH STYLE'S DATA INTO CONVERSATIONAL PROMPTS ===\n",
    "for df, name in zip(dfs, names):\n",
    "\n",
    "    # Use only the columns containing original and target responses\n",
    "    df = df[['response0', 'response1']]\n",
    "\n",
    "    # Split into training and testing sets (first 800 for training, rest for testing)\n",
    "    train_df = df.iloc[0:800].copy()\n",
    "    test_df = df.iloc[800:].copy()\n",
    "\n",
    "    # Function to format each row as a conversation for fine-tuning\n",
    "    def format_conversation(row):\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        f\"Rephrase the following customer service message to make it more {name}. \"\n",
    "                        \"Only output the rephrased message. Do not add any explanation or extra text.\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": row[\"response0\"]},  # Original response\n",
    "                {\"role\": \"assistant\", \"content\": row[\"response1\"]}  # Stylized (target) response\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # Apply formatting to all training and test rows and save them to the dictionary\n",
    "    raw_datasets_dict[name][\"train\"] = train_df.apply(format_conversation, axis=1).tolist()\n",
    "    raw_datasets_dict[name][\"test\"] = test_df.apply(format_conversation, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEFINE THE BASE MODEL TO BE USED ===\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"  # A small instruct-tuned version of LLaMA 3 from Unsloth\n",
    "\n",
    "# === LOAD TOKENIZER FROM THE PRETRAINED MODEL ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# === ENSURE A PAD TOKEN IS SET ===\n",
    "# Some models do not define a pad_token by default. Here, we set it to the eos_token.\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# === HANDLE CASES WITH UNREASONABLY LARGE MAX LENGTH ===\n",
    "# Some tokenizers might return an extremely high default max length.\n",
    "# If that's the case, reset it to a practical limit (2048 tokens).\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048\n",
    "\n",
    "# === DEFINE A CUSTOM CHAT TEMPLATE COMPATIBLE WITH UNSLOTH ===\n",
    "# This template formats a list of messages into a special format\n",
    "# used by Unsloth for training or inference in chat-based models.\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"\"\"{% for message in messages %}\n",
    "{% if message['role'] == 'system' %}\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "{{ message['content'] }}<|eot_id|>\n",
    "{% elif message['role'] == 'user' %}\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{{ message['content'] }}<|eot_id|>\n",
    "{% elif message['role'] == 'assistant' %}\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "{{ message['content'] }}<|eot_id|>\n",
    "{% endif %}\n",
    "{% if loop.last and add_generation_prompt %}\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "{% endif %}\n",
    "{% endfor %}\"\"\"\n",
    "\n",
    "# === APPLY THE CUSTOM CHAT TEMPLATE TO THE TOKENIZER ===\n",
    "# This ensures that when passing conversational data, the tokenizer will format it properly.\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FUNCTION TO APPLY THE CHAT TEMPLATE TO EACH EXAMPLE ===\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "\n",
    "    # If the first message isn't from the system, insert an empty system prompt\n",
    "    # This ensures consistency with the expected format (system â†’ user â†’ assistant)\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "\n",
    "    # Apply the chat template using the tokenizer (produces a formatted string)\n",
    "    # We do not tokenize it yet â€” just format into a single text field\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example\n",
    "\n",
    "def tokenize_for_causal_lm(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# === CONVERT RAW PYTHON LISTS INTO HUGGING FACE DATASETS ===\n",
    "# Each dataset corresponds to a specific style: clear, friendly, empathetic, polite\n",
    "raw_train_clear = Dataset.from_list(raw_datasets_dict[\"clear\"][\"train\"])\n",
    "raw_train_friendly = Dataset.from_list(raw_datasets_dict[\"friendly\"][\"train\"])\n",
    "raw_train_empathetic = Dataset.from_list(raw_datasets_dict[\"empathetic\"][\"train\"])\n",
    "raw_train_polite = Dataset.from_list(raw_datasets_dict[\"polite\"][\"train\"])\n",
    "\n",
    "# === GROUP ALL DATASETS INTO A LIST FOR ITERATION ===\n",
    "raw_datasets_list = [raw_train_clear, raw_train_friendly, raw_train_empathetic, raw_train_polite]\n",
    "\n",
    "# === INITIALIZE A DICTIONARY TO STORE THE FINAL FORMATTED TRAINING DATASETS ===\n",
    "train_dataset_dict = {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None}\n",
    "\n",
    "# === APPLY THE CHAT TEMPLATE TO ALL DATASETS IN PARALLEL ===\n",
    "for raw_dataset, name in zip(raw_datasets_list, names):\n",
    "\n",
    "    # Get the column names (not used further here, but useful for debugging or extensions)\n",
    "    column_names = list(raw_dataset.features)\n",
    "\n",
    "    # Placeholder for any columns to be removed (not used yet)\n",
    "    remove_columns = None\n",
    "\n",
    "    # Step 1: apply chat template\n",
    "    chat_formatted = raw_dataset.map(\n",
    "        apply_chat_template,\n",
    "        num_proc=cpu_count(),\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        desc=\"Applying chat template\"\n",
    "    )\n",
    "\n",
    "    # Step 2: tokenize for causal LM\n",
    "    train_dataset_dict[name] = chat_formatted.map(\n",
    "        tokenize_for_causal_lm,\n",
    "        num_proc=cpu_count(),\n",
    "        desc=\"Tokenizing text for causal LM\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# === SET UP QUANTIZATION CONFIGURATION ===\n",
    "# Load the model in 4-bit precision to save memory and improve speed (if supported)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4: more efficient quantization\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",  # Use bfloat16 during computation for better accuracy\n",
    ")\n",
    "\n",
    "# Determine the device map: use GPU if available\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "# === ADDITIONAL MODEL ARGUMENTS ===\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"flash_attention_2\",  # Use Flash Attention if available (speeds up attention computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False,  # Disable caching for training (gradient checkpointing)\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# GENERATION OF BASELINE EXAMPLES WITH NON-FINE-TUNED MODEL\n",
    "\n",
    "# === INITIALIZE EMPTY DICTIONARY TO STORE PREDICTIONS FOR EACH STYLE ===\n",
    "baseline_preds = {\n",
    "    \"clear\": [],\n",
    "    \"friendly\": [],\n",
    "    \"empathetic\": [],\n",
    "    \"polite\": []\n",
    "}\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "\n",
    "# Load quantized model for causal language modeling\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# === INFERENCE LOOP FOR EACH STYLE (clear, friendly, empathetic, polite) ===\n",
    "for name in names:\n",
    "    # Retrieve the list of test examples for this style\n",
    "    test_set_list = [item for item in raw_datasets_dict[name][\"test\"]]\n",
    "\n",
    "    # Iterate through each test example\n",
    "    for i, entry in enumerate(test_set_list):\n",
    "        # Use only the system and user messages as input to the model\n",
    "        messages = entry[\"messages\"][:2]\n",
    "\n",
    "        # Tokenize the conversation prompt\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            truncation=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Generate a response using sampling\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=40,       # Limit response length\n",
    "            do_sample=True,          # Use sampling instead of greedy decoding\n",
    "            temperature=1.0,         # Sampling temperature\n",
    "            top_p=1.0                # Top-p sampling (p=1.0 = no restriction)\n",
    "        )\n",
    "\n",
    "        # Decode generated tokens into text\n",
    "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "        # Extract the assistant's response (after the assistant header)\n",
    "        predicted_reply = generated_text.split(\"\\nassistant\\n\")[1].strip()\n",
    "\n",
    "        # Store the prediction in the dictionary\n",
    "        baseline_preds[name].append({\n",
    "            \"id\": i,\n",
    "            \"response0\": messages[1][\"content\"],  # Original user input\n",
    "            \"generated_response1\": predicted_reply  # Model-generated stylized output\n",
    "        })\n",
    "\n",
    "# === SAVE ALL GENERATED PREDICTIONS TO A JSON FILE ===\n",
    "with open(\"baseline_predictions.json\", \"w\") as f:\n",
    "    json.dump(baseline_preds, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE-TUNING OF MODEL AND GENERATION OF EXAMPLES WITH FINE-TUNED MODEL\n",
    "\n",
    "# === INITIALIZE A DICTIONARY TO STORE MODEL PREDICTIONS FOR EACH STYLE ===\n",
    "predictions = {\"clear\": [], \"friendly\": [], \"empathetic\": [], \"polite\": []}\n",
    "\n",
    "# === LOOP OVER EACH STYLE NAME ===\n",
    "for name in names:\n",
    "\n",
    "    # Define the output directory where checkpoints and logs will be stored\n",
    "    output_dir = f'data/Llama-3.2-1B-Instruct/{name}'\n",
    "\n",
    "    # === TRAINING ARGUMENTS ===\n",
    "    training_args = TrainingArguments(\n",
    "        fp16=True,  # Use half-precision training for speed and memory efficiency (use bf16=True on supporting GPUs)\n",
    "        do_eval=False,  # No evaluation during training\n",
    "        report_to=\"none\",  # Disable logging to external services like wandb\n",
    "        eval_strategy=\"no\",\n",
    "        gradient_accumulation_steps=256,  # Accumulate gradients over many steps (needed due to small batch size)\n",
    "        gradient_checkpointing=True,  # Enable memory-efficient training via checkpointing\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Required by some models for compatibility\n",
    "        learning_rate=2e-5,  # Standard fine-tuning learning rate\n",
    "        log_level=\"info\",  # Logging level\n",
    "        logging_steps=5,  # Log every 5 steps\n",
    "        logging_strategy=\"steps\",\n",
    "        lr_scheduler_type=\"cosine\",  # Cosine learning rate scheduler\n",
    "        max_steps=-1,  # No manual step limit; use `num_train_epochs`\n",
    "        num_train_epochs=1,  # Only one epoch (changeable)\n",
    "        output_dir=output_dir,  # Directory to store model output\n",
    "        overwrite_output_dir=True,  # Overwrite if already exists\n",
    "        per_device_eval_batch_size=1,  # Evaluation batch size (set small for memory safety)\n",
    "        per_device_train_batch_size=1,  # Training batch size (set small for memory safety)\n",
    "        save_strategy=\"no\",  # Do not save intermediate checkpoints\n",
    "        save_total_limit=None,\n",
    "        seed=42,  # Set random seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # === CONFIGURE PEFT (LoRA) FOR MEMORY-EFFICIENT FINETUNING ===\n",
    "    peft_config = LoraConfig(\n",
    "        r=64,  # Rank for low-rank adaptation\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",  # Task type is causal language modeling\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target transformer attention modules\n",
    "    )\n",
    "\n",
    "    # === LOAD BASE MODEL AND PREPARE FOR PEFT ===\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # === INITIALIZE THE SFT TRAINER ===\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,  # PEFT-wrapped model\n",
    "        args=training_args,  # TrainingArguments object\n",
    "        train_dataset=train_dataset_dict[name],  # Tokenized training data\n",
    "        processing_class=tokenizer,  # Tokenizer used for processing\n",
    "    )\n",
    "\n",
    "    # === START TRAINING ===\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # === SAVE THE FINE-TUNED MODEL TO DISK ===\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # === LOG AND SAVE METRICS ===\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = len(train_dataset_dict[name])\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset_dict[name]))\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    # === FINE-TUNED MODEL FOR INFERENCE ===\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        output_dir,\n",
    "        **model_kwargs\n",
    "    )\n",
    "\n",
    "    # === MERGE TRAIN AND TEST SETS FOR INFERENCE ===\n",
    "    train_set_list = [item for item in raw_datasets_dict[name][\"train\"]]\n",
    "    test_set_list = [item for item in raw_datasets_dict[name][\"test\"]]\n",
    "    complete_dataset_list = train_set_list + test_set_list\n",
    "\n",
    "    # === GENERATE PREDICTIONS FOR THE ENTIRE SET ===\n",
    "    for i, entry in enumerate(complete_dataset_list):\n",
    "        messages = entry[\"messages\"][:2]  # Only system and user messages\n",
    "\n",
    "        # Tokenize the conversation prompt\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            truncation=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Generate a response from the model\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "        )\n",
    "\n",
    "        # Decode generated output\n",
    "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "        # Extract only the assistant's reply (after the assistant role marker)\n",
    "        predicted_reply = generated_text.split(\"\\nassistant\\n\")[1].strip()\n",
    "\n",
    "        # Save the prediction\n",
    "        predictions[name].append({\n",
    "            \"id\": i,\n",
    "            \"response0\": messages[1][\"content\"],  # Original input\n",
    "            \"generated_response1\": predicted_reply  # Model output\n",
    "        })\n",
    "\n",
    "# === SAVE ALL PREDICTIONS TO A JSON FILE ===\n",
    "with open(\"sft_predictions.json\", \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Task with DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"HF_TOKEN\"] = \"HF_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RETRIEVE PARQUET FILES ===\n",
    "# Each path points to a different stylistic dataset\n",
    "path_clear = \"RESPONSible Service/dataset_clear.parquet\"\n",
    "path_friendly = \"RESPONSible Service/dataset_friendly.parquet\"\n",
    "path_empathetic = \"RESPONSible Service/dataset_empathetic.parquet\"\n",
    "path_polite = \"RESPONSible Service/dataset_polite.parquet\"\n",
    "\n",
    "# Read the Parquet content into Pandas DataFrames\n",
    "clear_df = pd.read_parquet(path_clear)\n",
    "friendly_df = pd.read_parquet(path_friendly)\n",
    "empathetic_df = pd.read_parquet(path_empathetic)\n",
    "polite_df = pd.read_parquet(path_polite)\n",
    "\n",
    "# === SHUFFLE THE DATAFRAMES ===\n",
    "# Shuffle the rows in each DataFrame to ensure randomness while keeping reproducibility with seed\n",
    "clear_df = clear_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "friendly_df = friendly_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "empathetic_df = empathetic_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "polite_df = polite_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === LOAD MODEL GENERATED RESPONSES ===\n",
    "# Load a JSON file containing model-generated alternatives to response1, called \"generated_response1\"\n",
    "with open(\"sft_predictions.json\", \"r\") as f:\n",
    "    sft_predictions = json.load(f)\n",
    "\n",
    "# === INITIALIZE STRUCTURES FOR PROCESSING ===\n",
    "dfs = [clear_df, friendly_df, empathetic_df, polite_df]  # List of datasets\n",
    "names = [\"clear\", \"friendly\", \"empathetic\", \"polite\"]    # Corresponding style names\n",
    "# Dictionary to hold train/test splits for each style\n",
    "raw_datasets_dict = {\n",
    "    \"clear\": {\"train\": None, \"test\": None},\n",
    "    \"friendly\": {\"train\": None, \"test\": None},\n",
    "    \"empathetic\": {\"train\": None, \"test\": None},\n",
    "    \"polite\": {\"train\": None, \"test\": None}\n",
    "}\n",
    "\n",
    "# === CREATE STRUCTURED DATASETS FOR EACH STYLE ===\n",
    "for df, name in zip(dfs, names):\n",
    "\n",
    "    # Extract model-generated alternative responses (\"response0\")\n",
    "    sft_generated_response1 = [item[\"generated_response1\"] for item in sft_predictions[name]]\n",
    "\n",
    "    # Keep only the original human-written 'response1' column\n",
    "    df = df[['response1']].copy()\n",
    "\n",
    "    # Add the model-generated 'response0' as the \"rejected\" response\n",
    "    df[\"response0\"] = sft_generated_response1\n",
    "\n",
    "    # Rename columns to match preference learning format\n",
    "    df = df.rename(columns={\"response0\": \"rejected\", \"response1\": \"chosen\"})\n",
    "\n",
    "    # Add a task-specific prompt to guide model rephrasing\n",
    "    df[\"prompt\"] = f\"Rephrase the following customer service message to make it more {name}. Only output the rephrased message. Do not add any explanation or extra text.\"\n",
    "\n",
    "    # Split into train (first 800 rows) and test (remaining rows)\n",
    "    raw_datasets_dict[name][\"train\"] = df.iloc[0:800].copy()\n",
    "    raw_datasets_dict[name][\"test\"] = df.iloc[800:].copy()\n",
    "\n",
    "# === CONVERT TRAINING DATAFRAMES INTO HUGGINGFACE DATASETS ===\n",
    "# Convert each training set from pandas to HuggingFace Dataset format\n",
    "train_clear = Dataset.from_pandas(raw_datasets_dict[\"clear\"][\"train\"])\n",
    "train_friendly = Dataset.from_pandas(raw_datasets_dict[\"friendly\"][\"train\"])\n",
    "train_empathetic = Dataset.from_pandas(raw_datasets_dict[\"empathetic\"][\"train\"])\n",
    "train_polite = Dataset.from_pandas(raw_datasets_dict[\"polite\"][\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of training datasets, one for each style (clear, friendly, empathetic, polite)\n",
    "train_sets = [train_clear, train_friendly, train_empathetic, train_polite]\n",
    "\n",
    "# Iterate over each style name and its corresponding dataset\n",
    "for name, train_set in zip(names, train_sets):\n",
    "\n",
    "    max_seq_length = 4096  # Maximum input sequence length for the model\n",
    "    dtype = None  # Data type left as default (can be set to torch.float16/torch.bfloat16 if needed)\n",
    "    load_in_4bit = True  # Enable 4-bit quantization to reduce memory footprint during training\n",
    "\n",
    "    # Load a quantized Llama 3.2-1B-Instruct model fine-tuned on a specific style\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = f\"data/Llama-3.2-1B-Instruct/{name}\",  # Path to the model fine-tuned on a specific style\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit\n",
    "    )\n",
    "\n",
    "    # Initialize a Direct Preference Optimization (DPO) trainer for reward modeling\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model = model,               # The policy model to be trained\n",
    "        ref_model = None,            # No reference model is used (can be used for KL divergence regularization)\n",
    "        args = DPOConfig(            # Training configuration\n",
    "            per_device_train_batch_size = 2,  # Batch size per device\n",
    "            gradient_accumulation_steps = 4,  # Effective batch size = 2 * 4 = 8\n",
    "            warmup_ratio = 0.1,               # Fraction of steps for LR warmup\n",
    "            num_train_epochs = 1,             # One epoch for quick experimentation\n",
    "            learning_rate = 5e-6,             # Learning rate\n",
    "            fp16 = not is_bfloat16_supported(),  # Use FP16 if BF16 is not supported\n",
    "            bf16 = is_bfloat16_supported(),      # Use BF16 if supported by hardware\n",
    "            logging_steps = 1,                   # Log every step (will be ignored since logging is disabled)\n",
    "            optim = \"adamw_8bit\",                # Optimizer with 8-bit weights for memory efficiency\n",
    "            weight_decay = 0.0,                  # No weight decay (L2 regularization)\n",
    "            lr_scheduler_type = \"linear\",        # Linear learning rate schedule\n",
    "            seed = 42,                           # Seed for reproducibility\n",
    "            output_dir = f\"DPO/{name}\",          # Directory to save model checkpoints and logs\n",
    "            report_to = \"none\",                  # No logging to external tools (e.g., WandB)\n",
    "            logging_strategy = \"no\",             # Disable logging\n",
    "            log_level = \"error\"                  # Suppress most logs\n",
    "        ),\n",
    "        beta = 0.1,                  # Temperature parameter for DPO loss\n",
    "        train_dataset = train_set,   # The specific dataset for the current style\n",
    "        tokenizer = tokenizer,       # Corresponding tokenizer\n",
    "        max_length = 1024,           # Max total length (prompt + response)\n",
    "        max_prompt_length = 512,     # Max length of the prompt portion\n",
    "    )\n",
    "\n",
    "    # Train the model using the DPO method on the given dataset\n",
    "    dpo_trainer.train()\n",
    "\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the predictions for each style\n",
    "dpo_predictions = {\"clear\": [], \"friendly\": [], \"empathetic\": [], \"polite\": []}\n",
    "\n",
    "# Iterate over each style name\n",
    "for name in names:\n",
    "\n",
    "    # === Load the DPO-trained model and tokenizer from checkpoint ===\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=f\"DPO/{name}/checkpoint-100\",  # Path to model checkpoint\n",
    "        max_seq_length=2048,                      # Max sequence length allowed\n",
    "        dtype=None,                               # Use default dtype\n",
    "        load_in_4bit=True                         # Load model using 4-bit quantization for memory efficiency\n",
    "    )\n",
    "\n",
    "    # Prepare model for inference (e.g., disables training-specific behavior)\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    # Get the device where the model is loaded (CPU or GPU)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Get test set messages (rejected responses) for the current style\n",
    "    user_messages = raw_datasets_dict[name][\"test\"][\"rejected\"]\n",
    "\n",
    "    # === Format prompts using the chat template ===\n",
    "    system_prompt = f\"Rephrase the following customer service message to make it more {name}. Only output the rephrased message. Do not add any explanation or extra text.\"\n",
    "\n",
    "    # Build a list of prompts with a system instruction and user message\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": msg}\n",
    "        ], add_generation_prompt=True, tokenize=False)\n",
    "        for msg in user_messages\n",
    "    ]\n",
    "\n",
    "    # === Tokenize all prompts in a single batch ===\n",
    "    batch = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}  # Move batch tensors to the same device as the model\n",
    "\n",
    "    # === Generate responses for all prompts ===\n",
    "    outputs = model.generate(\n",
    "        **batch,\n",
    "        do_sample=True,               # Enable sampling for more diverse outputs\n",
    "        temperature=1.0,              # Sampling temperature (controls randomness)\n",
    "        top_p=1.0,                    # Top-p nucleus sampling (1.0 means no restriction)\n",
    "        max_new_tokens=40,           # Limit the number of generated tokens\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Use EOS token for padding\n",
    "    )\n",
    "\n",
    "    # === Decode the generated outputs into human-readable text ===\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # === Extract and structure rephrased responses ===\n",
    "    for i, text in enumerate(decoded):\n",
    "        pred = {}\n",
    "        try:\n",
    "            # Separate the prompt and the generated response\n",
    "            text1 = text.split(\"user\\n\\n\")[1].strip('\\n\\r\"\\'')\n",
    "            text2 = text1.split(\"assistant\\n\\n\")\n",
    "            user = text2[0].strip('\\n\\r\"\\'')         # Original user input\n",
    "            assistant = text2[1].strip('\\n\\r\"\\'')    # Model-generated rephrased output\n",
    "\n",
    "            pred[\"id\"] = i\n",
    "            pred[\"response0\"] = user\n",
    "            pred[\"generated_response1\"] = assistant\n",
    "            predictions.append(pred)\n",
    "        except IndexError:\n",
    "            # In case the format doesn't match expectations, skip the sample\n",
    "            continue\n",
    "\n",
    "    # Store predictions for this particular style\n",
    "    dpo_predictions[name].extend(predictions)\n",
    "\n",
    "# === Optionally: Save the predictions to a JSON file ===\n",
    "with open(\"dpo_predictions.json\", \"w\") as f:\n",
    "    json.dump(dpo_predictions, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Evaluation\n",
    "\n",
    "The following section includes the script to process as .csv files the model outputs that were generated in the text generation tasks, so that they can be manually annotated and then re-uploaded to calculate human ground truth agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "def process_json_to_csv(json_file: str, output_folder: str = \"csv_outputs\", nested: bool = True):\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through top-level subsets\n",
    "    for style in data:\n",
    "        if nested:\n",
    "            # Handle nested structure: style â†’ shot level (e.g., \"0-shot\")\n",
    "            for shot in data[style]:\n",
    "                entries = data[style][shot]\n",
    "                df = pd.DataFrame(entries)[[\"response0\", \"generated_response1\"]]\n",
    "                df = df.sample(n=25, random_state=42).reset_index(drop=True)\n",
    "\n",
    "                # Swap responses randomly and assign labels\n",
    "                rows = []  # âœ… Fixed initialization\n",
    "                for _, row in df.iterrows():\n",
    "                    if random.random() < 0.5:\n",
    "                        rows.append({\"response0\": row[\"response0\"],\n",
    "                                     \"generated_response1\": row[\"generated_response1\"], \"label\": 1})\n",
    "                    else:\n",
    "                        rows.append({\"response0\": row[\"generated_response1\"],\n",
    "                                     \"generated_response1\": row[\"response0\"], \"label\": 0})\n",
    "                df_final = pd.DataFrame(rows)\n",
    "\n",
    "                # Separate label column\n",
    "                labels = df_final[\"label\"]\n",
    "                df_final = df_final.drop(columns=[\"label\"])\n",
    "                df_final[\"human\"] = \"\"\n",
    "\n",
    "                # Save main CSV (without label)\n",
    "                filename = f\"{style}_{shot}.csv\"\n",
    "                df_final.to_csv(os.path.join(output_folder, filename), index=False, encoding=\"utf-8\", sep=\";\")\n",
    "\n",
    "                # Save label CSV\n",
    "                label_filename = f\"{style}_{shot}_labels.csv\"\n",
    "                pd.DataFrame({\"label\": labels}).to_csv(os.path.join(output_folder, label_filename), index=False, encoding=\"utf-8\", sep=\";\")\n",
    "\n",
    "        else:\n",
    "            # Handle flat structure: style â†’ list of entries\n",
    "            entries = data[style]\n",
    "            df = pd.DataFrame(entries)[[\"response0\", \"generated_response1\"]]\n",
    "            df = df.sample(n=25, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            # Swap responses randomly and assign labels\n",
    "            rows = []  # âœ… Fixed initialization\n",
    "            for _, row in df.iterrows():\n",
    "                if random.random() < 0.5:\n",
    "                    rows.append({\"response0\": row[\"response0\"],\n",
    "                                 \"generated_response1\": row[\"generated_response1\"], \"label\": 1})\n",
    "                else:\n",
    "                    rows.append({\"response0\": row[\"generated_response1\"],\n",
    "                                 \"generated_response1\": row[\"response0\"], \"label\": 0})\n",
    "            df_final = pd.DataFrame(rows)\n",
    "\n",
    "            # Separate label column\n",
    "            labels = df_final[\"label\"]\n",
    "            df_final = df_final.drop(columns=[\"label\"])\n",
    "            df_final[\"human\"] = \"\"\n",
    "\n",
    "            # Save main CSV (without label)\n",
    "            filename = f\"{style}.csv\"\n",
    "            df_final.to_csv(os.path.join(output_folder, filename), index=False, encoding=\"utf-8\", sep=\";\")\n",
    "\n",
    "            # Save label CSV\n",
    "            label_filename = f\"{style}_labels.csv\"\n",
    "            pd.DataFrame({\"label\": labels}).to_csv(os.path.join(output_folder, label_filename), index=False, encoding=\"utf-8\", sep=\";\")\n",
    "\n",
    "    print(\"âœ… CSV export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_json_to_csv(\"icl_predictions.json\", output_folder=\"hum_eval_icl\", nested=True)\n",
    "process_json_to_csv(\"sft_predictions.json\", output_folder=\"hum_eval_sft\", nested=False)\n",
    "process_json_to_csv(\"dpo_predictions.json\", output_folder=\"hum_eval_dpo\", nested=False)\n",
    "process_json_to_csv(\"baseline_predictions.json\", output_folder=\"hum_eval_baseline\", nested=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_agreement(data_file: str, label_file: str) -> float:\n",
    "    # Load main CSV with human annotations\n",
    "    df_data = pd.read_csv(data_file, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "    # Load label CSV with model-assigned labels\n",
    "    df_labels = pd.read_csv(label_file, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "    # Ensure both files have the same number of rows\n",
    "    if len(df_data) != len(df_labels):\n",
    "        raise ValueError(\"Files do not match in number of rows.\")\n",
    "\n",
    "    # Extract human and label columns as integer lists\n",
    "    human = df_data[\"human\"].astype(str).str.strip()\n",
    "    label = df_labels[\"label\"].astype(str).str.strip()\n",
    "\n",
    "    # Filter out missing human annotations (empty strings or NaN)\n",
    "    valid_idx = (human != \"\") & (human != \"nan\")\n",
    "\n",
    "    human = human[valid_idx].astype(int).tolist()\n",
    "    label = label[valid_idx].astype(int).tolist()\n",
    "\n",
    "    if not human:\n",
    "        print(\"No human annotations found.\")\n",
    "        return 0.0\n",
    "\n",
    "    # Compute agreement\n",
    "    matches = sum(h == l for h, l in zip(human, label))\n",
    "    agreement_percent = matches / len(human) * 100\n",
    "\n",
    "    return round(agreement_percent, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE YOU RUN THIS CELL, MAKE SURE YOU HAVE COMPLETED THE FOLLOWING PASSAGES:\n",
    "    # 1. EXPORT THE.csv FILES (the ones not ending with _label)\n",
    "    # 2. MANUALLY ADD THE HUMAN LABELS\n",
    "    # 3. REPLACE THE dataset_{style}.csv FILES WITH THE MANUALLY ANNOTATED ONES (KEEP THE SAME FILE NAME)\n",
    "\n",
    "from scipy.stats import hmean\n",
    "import pandas as pd\n",
    "\n",
    "directories = [\"hum_eval_icl\", \"hum_eval_sft\", \"hum_eval_dpo\", \"hum_eval_baseline\"]\n",
    "settings = [\"icl\", \"sft\", \"dpo\", \"baseline\"]\n",
    "shots = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "subsets = [\"clear\", 'friendly', \"empathetic\", \"polite\"]\n",
    "\n",
    "# Initialize results\n",
    "results = {\n",
    "    \"baseline\": {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None},\n",
    "    \"sft\": {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None},\n",
    "    \"dpo\": {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None},\n",
    "    \"icl-0\": {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None},\n",
    "    \"icl-1\": {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None},\n",
    "    \"icl-2\": {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None},\n",
    "    \"icl-3\": {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None},\n",
    "    \"icl-4\": {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None},\n",
    "    \"icl-5\": {\"clear\": None, \"friendly\": None, \"empathetic\": None, \"polite\": None}\n",
    "}\n",
    "\n",
    "# Fill in results\n",
    "for setting, directory in zip(settings, directories):\n",
    "    for subset in subsets:\n",
    "        if directory == \"hum_eval_icl\":\n",
    "            for shot in shots:\n",
    "                csv1 = f\"{directory}/{subset}_{shot}-shot.csv\"\n",
    "                csv2 = f\"{directory}/{subset}_{shot}-shot_labels.csv\"\n",
    "                agreement_score = compute_agreement(csv1, csv2)\n",
    "                results[f\"{setting}-{shot}\"][subset] = agreement_score\n",
    "        else:\n",
    "            csv1 = f\"{directory}/{subset}.csv\"\n",
    "            csv2 = f\"{directory}/{subset}_labels.csv\"\n",
    "            agreement_score = compute_agreement(csv1, csv2)\n",
    "            results[setting][subset] = agreement_score\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "# Compute harmonic mean across each row\n",
    "df[\"HM\"] = df.apply(lambda row: hmean([v for v in row if v is not None and v > 0]), axis=1)\n",
    "\n",
    "# Compute harmonic mean across each column\n",
    "hmean_row = df.apply(lambda col: hmean([v for v in col if v is not None and v > 0]), axis=0)\n",
    "hmean_row.name = \"HM\"\n",
    "\n",
    "# Append row\n",
    "df = pd.concat([df, pd.DataFrame([hmean_row])], axis=0)\n",
    "\n",
    "# Move method index into a column\n",
    "df = df.reset_index().rename(columns={\"index\": \"method\"})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"texgen_human_agreement.csv\", encoding=\"utf-8\", sep=\";\", index=False)\n",
    "\n",
    "# Display final DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap between Predictions and Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements_3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === Load model-generated predictions from JSON files ===\n",
    "\n",
    "# Load ICL (In-Context Learning) predictions\n",
    "with open('icl_predictions.json', 'r') as f:\n",
    "    icl_predictions = json.load(f)\n",
    "\n",
    "# Load baseline model predictions (non-finetuned model)\n",
    "with open('baseline_predictions.json', 'r') as f:\n",
    "    baseline_predictions = json.load(f)\n",
    "\n",
    "# Load SFT (Supervised Fine-Tuning) predictions\n",
    "with open('sft_predictions.json', 'r') as f:\n",
    "    sft_predictions = json.load(f)\n",
    "\n",
    "# Load DPO (Direct Preference Optimization) predictions\n",
    "with open('dpo_predictions.json', 'r') as f:\n",
    "    dpo_predictions = json.load(f)\n",
    "\n",
    "# === Retrieve the test datasets for each stylistic category ===\n",
    "\n",
    "# Each path points to a different stylistic dataset\n",
    "path_clear = \"RESPONSible Service/dataset_clear.parquet\"\n",
    "path_friendly = \"RESPONSible Service/dataset_friendly.parquet\"\n",
    "path_empathetic = \"RESPONSible Service/dataset_empathetic.parquet\"\n",
    "path_polite = \"RESPONSible Service/dataset_polite.parquet\"\n",
    "\n",
    "# Read the Parquet content into Pandas DataFrames\n",
    "clear_df = pd.read_parquet(path_clear)\n",
    "friendly_df = pd.read_parquet(path_friendly)\n",
    "empathetic_df = pd.read_parquet(path_empathetic)\n",
    "polite_df = pd.read_parquet(path_polite)\n",
    "\n",
    "# === Shuffle datasets to ensure randomness before test/train split ===\n",
    "\n",
    "# Reproducible shuffling using fixed seed (42)\n",
    "clear_df = clear_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "friendly_df = friendly_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "empathetic_df = empathetic_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "polite_df = polite_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === Split into test sets ===\n",
    "\n",
    "# Take the last 200 rows as the test set (assuming original size â‰¥ 1000)\n",
    "clear_test = clear_df.iloc[800:].reset_index(drop=True)\n",
    "friendly_test = friendly_df.iloc[800:].reset_index(drop=True)\n",
    "empathetic_test = empathetic_df.iloc[800:].reset_index(drop=True)\n",
    "polite_test = polite_df.iloc[800:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clear = pd.DataFrame({\n",
    "    'gt': clear_test[\"response1\"],\n",
    "    'icl-0': [entry['generated_response1'] for entry in icl_predictions[\"clear\"]['0-shot']],\n",
    "    'icl-1': [entry['generated_response1'] for entry in icl_predictions[\"clear\"]['1-shot']],\n",
    "    'icl-2': [entry['generated_response1'] for entry in icl_predictions[\"clear\"]['2-shot']],\n",
    "    'icl-3': [entry['generated_response1'] for entry in icl_predictions[\"clear\"]['3-shot']],\n",
    "    'icl-4': [entry['generated_response1'] for entry in icl_predictions[\"clear\"]['4-shot']],\n",
    "    'icl-5': [entry['generated_response1'] for entry in icl_predictions[\"clear\"]['5-shot']],\n",
    "    'baseline': [entry['generated_response1'] for entry in baseline_predictions[\"clear\"]],\n",
    "    'sft': [entry['generated_response1'] for entry in sft_predictions[\"clear\"][800:]],\n",
    "    'dpo': [entry['generated_response1'] for entry in dpo_predictions[\"clear\"]]\n",
    "})\n",
    "\n",
    "df_friendly = pd.DataFrame({\n",
    "    'gt': friendly_test[\"response1\"],\n",
    "    'icl-0': [entry['generated_response1'] for entry in icl_predictions[\"friendly\"]['0-shot']],\n",
    "    'icl-1': [entry['generated_response1'] for entry in icl_predictions[\"friendly\"]['1-shot']],\n",
    "    'icl-2': [entry['generated_response1'] for entry in icl_predictions[\"friendly\"]['2-shot']],\n",
    "    'icl-3': [entry['generated_response1'] for entry in icl_predictions[\"friendly\"]['3-shot']],\n",
    "    'icl-4': [entry['generated_response1'] for entry in icl_predictions[\"friendly\"]['4-shot']],\n",
    "    'icl-5': [entry['generated_response1'] for entry in icl_predictions[\"friendly\"]['5-shot']],\n",
    "    'baseline': [entry['generated_response1'] for entry in baseline_predictions[\"friendly\"]],\n",
    "    'sft': [entry['generated_response1'] for entry in sft_predictions[\"friendly\"][800:]],\n",
    "    'dpo': [entry['generated_response1'] for entry in dpo_predictions[\"friendly\"]]\n",
    "})\n",
    "\n",
    "df_empathetic = pd.DataFrame({\n",
    "    'gt': empathetic_test[\"response1\"],\n",
    "    'icl-0': [entry['generated_response1'] for entry in icl_predictions[\"empathetic\"]['0-shot']],\n",
    "    'icl-1': [entry['generated_response1'] for entry in icl_predictions[\"empathetic\"]['1-shot']],\n",
    "    'icl-2': [entry['generated_response1'] for entry in icl_predictions[\"empathetic\"]['2-shot']],\n",
    "    'icl-3': [entry['generated_response1'] for entry in icl_predictions[\"empathetic\"]['3-shot']],\n",
    "    'icl-4': [entry['generated_response1'] for entry in icl_predictions[\"empathetic\"]['4-shot']],\n",
    "    'icl-5': [entry['generated_response1'] for entry in icl_predictions[\"empathetic\"]['5-shot']],\n",
    "    'baseline': [entry['generated_response1'] for entry in baseline_predictions[\"empathetic\"]],\n",
    "    'sft': [entry['generated_response1'] for entry in sft_predictions[\"empathetic\"][800:]],\n",
    "    'dpo': [entry['generated_response1'] for entry in dpo_predictions[\"empathetic\"]]\n",
    "})\n",
    "\n",
    "df_polite = pd.DataFrame({\n",
    "    'gt': polite_test[\"response1\"],\n",
    "    'icl-0': [entry['generated_response1'] for entry in icl_predictions[\"polite\"]['0-shot']],\n",
    "    'icl-1': [entry['generated_response1'] for entry in icl_predictions[\"polite\"]['1-shot']],\n",
    "    'icl-2': [entry['generated_response1'] for entry in icl_predictions[\"polite\"]['2-shot']],\n",
    "    'icl-3': [entry['generated_response1'] for entry in icl_predictions[\"polite\"]['3-shot']],\n",
    "    'icl-4': [entry['generated_response1'] for entry in icl_predictions[\"polite\"]['4-shot']],\n",
    "    'icl-5': [entry['generated_response1'] for entry in icl_predictions[\"polite\"]['5-shot']],\n",
    "    'baseline': [entry['generated_response1'] for entry in baseline_predictions[\"polite\"]],\n",
    "    'sft': [entry['generated_response1'] for entry in sft_predictions[\"polite\"][800:]],\n",
    "    'dpo': [entry['generated_response1'] for entry in dpo_predictions[\"polite\"]]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Overlap (BERTScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert_score import score as bertscore\n",
    "from scipy.stats import shapiro, f_oneway, kruskal\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Your pre-defined DataFrames: df_clear, df_friendly, df_empathetic, df_polite\n",
    "dfs = {\n",
    "    \"clear\": df_clear,\n",
    "    \"friendly\": df_friendly,\n",
    "    \"empathetic\": df_empathetic,\n",
    "    \"polite\": df_polite\n",
    "}\n",
    "\n",
    "# Short tags for conditions\n",
    "abbr = {\n",
    "    \"icl-0\": \"0\", \"icl-1\": \"1\", \"icl-2\": \"2\", \"icl-3\": \"3\",\n",
    "    \"icl-4\": \"4\", \"icl-5\": \"5\", \"baseline\": \"b\",\n",
    "    \"sft\": \"s\", \"dpo\": \"d\"\n",
    "}\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "all_reports = {}\n",
    "raw_f1_scores = {}\n",
    "test_info = {}  # Store test type and p-value per style\n",
    "\n",
    "# Compute BERTScore\n",
    "for style, df in dfs.items():\n",
    "    references = df[\"gt\"].tolist()\n",
    "    report = {}\n",
    "    raw_f1_scores[style] = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col == \"gt\":\n",
    "            continue\n",
    "        candidates = df[col].tolist()\n",
    "        P, R, F1 = bertscore(\n",
    "            candidates, references,\n",
    "            lang='en',\n",
    "            model_type='bert-base-uncased',\n",
    "            device=DEVICE,\n",
    "            verbose=False\n",
    "        )\n",
    "        f1_list = F1.numpy().tolist()\n",
    "        raw_f1_scores[style][col] = f1_list\n",
    "        report[col] = {\n",
    "            \"precision\": np.mean(P.numpy()),\n",
    "            \"recall\": np.mean(R.numpy()),\n",
    "            \"f1\": np.mean(F1.numpy())\n",
    "        }\n",
    "\n",
    "    all_reports[style] = pd.DataFrame(report).T\n",
    "\n",
    "# Add significance column and test type\n",
    "bert_results = {}\n",
    "for style, df in all_reports.items():\n",
    "    f1_data = raw_f1_scores[style]\n",
    "    keys = list(f1_data.keys())\n",
    "    values = list(f1_data.values())\n",
    "\n",
    "    # Shapiro-Wilk normality test\n",
    "    shapiro_p = [shapiro(v).pvalue for v in values]\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p)\n",
    "\n",
    "    # Global test\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"\n",
    "        stat, p_val = f_oneway(*values)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"\n",
    "        stat, p_val = kruskal(*values)\n",
    "\n",
    "    test_info[style] = f\"{test_type} (p = {p_val:.4f})\"\n",
    "\n",
    "    SS = []\n",
    "    if p_val < 0.05:\n",
    "        # Post-hoc test\n",
    "        df_long = pd.DataFrame({\n",
    "            \"score\": [v for sublist in values for v in sublist],\n",
    "            \"label\": sum([[k]*len(f1_data[k]) for k in keys], [])\n",
    "        })\n",
    "        posthoc = sp.posthoc_dunn(df_long, val_col=\"score\", group_col=\"label\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        for row in df.index:\n",
    "            sig_with = [\n",
    "                abbr[col] for col in df.index\n",
    "                if col != row and posthoc.loc[row, col] < 0.05\n",
    "            ]\n",
    "            if len(sig_with) == len(df.index) - 1:\n",
    "                SS.append(\"*\")\n",
    "            elif len(sig_with) == 0:\n",
    "                SS.append(\"-\")\n",
    "            else:\n",
    "                SS.append(\"\".join(sorted(sig_with)))\n",
    "    else:\n",
    "        SS = [\"-\"] * len(df)\n",
    "\n",
    "    df[\"SS\"] = SS\n",
    "    bert_results[style] = df.round(2)\n",
    "\n",
    "# Write to file\n",
    "with open(\"texgen_bertscores.txt\", \"w\") as f:\n",
    "    for style, df in bert_results.items():\n",
    "        f.write(f\"=== {style.upper()} ===\\n\")\n",
    "        f.write(f\"Statistical Test Used: {test_info[style]}\\n\")\n",
    "        f.write(df.to_string())\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Overlap (ROUGE-L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from scipy.stats import shapiro, f_oneway, kruskal\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Your pre-defined DataFrames: df_clear, df_friendly, df_empathetic, df_polite\n",
    "dfs = {\n",
    "    \"clear\": df_clear,\n",
    "    \"friendly\": df_friendly,\n",
    "    \"empathetic\": df_empathetic,\n",
    "    \"polite\": df_polite\n",
    "}\n",
    "\n",
    "# Short tags for conditions\n",
    "abbr = {\n",
    "    \"icl-0\": \"0\", \"icl-1\": \"1\", \"icl-2\": \"2\", \"icl-3\": \"3\",\n",
    "    \"icl-4\": \"4\", \"icl-5\": \"5\", \"baseline\": \"b\",\n",
    "    \"sft\": \"s\", \"dpo\": \"d\"\n",
    "}\n",
    "\n",
    "all_reports = {}\n",
    "raw_rouge_scores = {}\n",
    "test_info = {}  # Store test type and p-value per style\n",
    "\n",
    "# Function to compute ROUGE-L F1 scores for each request/response pair\n",
    "def compute_rouge_l_scores(references, candidates):\n",
    "    # Initialize ROUGE-L scorer with stemming enabled\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    # Compute ROUGE-L F1 score for each (reference, candidate) pair\n",
    "    return [\n",
    "        scorer.score(ref, cand)['rougeL'].fmeasure\n",
    "        for ref, cand in zip(references, candidates)\n",
    "    ]\n",
    "\n",
    "# Compute ROUGE-L scores\n",
    "for style, df in dfs.items():\n",
    "    references = df[\"gt\"].tolist()\n",
    "    report = {}\n",
    "    raw_rouge_scores[style] = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col == \"gt\":\n",
    "            continue\n",
    "        candidates = df[col].tolist()\n",
    "\n",
    "        # Compute ROUGE-L F1 scores\n",
    "        rouge_l_scores = compute_rouge_l_scores(references, candidates)\n",
    "        raw_rouge_scores[style][col] = rouge_l_scores\n",
    "\n",
    "        report[col] = {\n",
    "            \"rouge_l\": np.mean(rouge_l_scores)\n",
    "        }\n",
    "\n",
    "    all_reports[style] = pd.DataFrame(report).T\n",
    "\n",
    "# Add significance column and test type\n",
    "rouge_results = {}\n",
    "for style, df in all_reports.items():\n",
    "    rouge_data = raw_rouge_scores[style]\n",
    "    keys = list(rouge_data.keys())\n",
    "    values = list(rouge_data.values())\n",
    "\n",
    "    # Shapiro-Wilk normality test\n",
    "    shapiro_p = [shapiro(v).pvalue for v in values]\n",
    "    all_normal = all(p >= 0.05 for p in shapiro_p)\n",
    "\n",
    "    # Global test\n",
    "    if all_normal:\n",
    "        test_type = \"ANOVA\"\n",
    "        stat, p_val = f_oneway(*values)\n",
    "    else:\n",
    "        test_type = \"Kruskal-Wallis\"\n",
    "        stat, p_val = kruskal(*values)\n",
    "\n",
    "    test_info[style] = f\"{test_type} (p = {p_val:.4f})\"\n",
    "\n",
    "    SS = []\n",
    "    if p_val < 0.05:\n",
    "        # Post-hoc test\n",
    "        df_long = pd.DataFrame({\n",
    "            \"score\": [v for sublist in values for v in sublist],\n",
    "            \"label\": sum([[k]*len(rouge_data[k]) for k in keys], [])\n",
    "        })\n",
    "        posthoc = sp.posthoc_dunn(df_long, val_col=\"score\", group_col=\"label\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        for row in df.index:\n",
    "            sig_with = [\n",
    "                abbr[col] for col in df.index\n",
    "                if col != row and posthoc.loc[row, col] < 0.05\n",
    "            ]\n",
    "            if len(sig_with) == len(df.index) - 1:\n",
    "                SS.append(\"*\")\n",
    "            elif len(sig_with) == 0:\n",
    "                SS.append(\"-\")\n",
    "            else:\n",
    "                SS.append(\"\".join(sorted(sig_with)))\n",
    "    else:\n",
    "        SS = [\"-\"] * len(df)\n",
    "\n",
    "    df[\"SS\"] = SS\n",
    "    rouge_results[style] = df.round(3)  # 3 decimal places for ROUGE-L scores\n",
    "\n",
    "# Write to file\n",
    "with open(\"texgen_rouge_scores.txt\", \"w\") as f:\n",
    "    for style, df in rouge_results.items():\n",
    "        f.write(f\"=== {style.upper()} ===\\n\")\n",
    "        f.write(f\"Statistical Test Used: {test_info[style]}\\n\")\n",
    "        f.write(df.to_string())\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
